\chapter{Results}
\label{chapter:results}

%5.0 Yleistä tuloksista
We first calculated internal and external validation metrics for 
the manually annotated data to see how well our metrics 
reveal our defined ground truth clustering. Then we clustered 
the actual data for one year with selected number of cluster 
values $n$, and calculated internal validation metrics for
each clustering. Based on metrics we choose the number of clusters
as our solution to the problem and we inspect the resulting
clustering.

%5.1 Manually annotated data  --> the chosen metrics
%5.2 Metrics evaluation, also compare to precision and recall??
%5.3 Chosen number of clusters based on metrics (oikeasti tiedetään: 3)
%5.4 Klusterointi kolmeen
%5.5 Klustereiden havainnollistaminen
\section{Manually annotated data}
We evaluated our clustering by creating an evaluation set with
hand labeled or verified fields of science. This subset of data 
consisted of 519 publications from three different fields. The fields
were: \emph{Computer science: Information systems}, 147 publications,
\emph{Computer science: Artificial intelligence} 153 publications, 
and \emph{Clinical neurology} 250 publications as classified by 
publisher. We checked the publisher labeling, cleared the 
bad samples lacking enough meta data, being too far out of the 
scope of any of the three disciplines or having corrupted data and
relabeled few \fixme{(amount)} samples to the more fitting 
discipline and to one discipline for each sample only. The ground 
truth labeling for 37 the most ambiguous samples was checked by 
a non-expert human reviewer. After clearing the data set consisted 
of 455 samples.  So, our manual classification will
be subjective but gives some hint about the clustering performance.
\fixme{Then we calculated precision and recall for the data set.} 

The Figure \ref{fig:ch-silh01} shows the results. We see that
Calinski-Harabasz index reaches it's maximum value at the number
of clusters two and that silhouette values increases with the
number of clusters.
Neither of these indices corresponds to the three disciplines
that we selected manually. It's also worth to remember that there
is only 455 data points to cluster. Comparison against preferred
manual labeling is calculated with the 
Adjusted Rand Index (ARI) for these clusterings in Figure 
\ref{fig:ari01}.
Top terms for maximum index values are shown in the Table 
\ref{refhere}.


\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=9.5cm]{images/c-h-silh-index-plot-519-2_260-800-hierarchical.png}
    \caption{Hierarchical clustering. Calinski-Harabasz index and Silhoutte values for the
    manually annotated set of 455 publications clustered with hierarchical
    clustering, LSA with 800 components. The higher values denote 
    more compact and better clustering in both graphs.}
    \label{fig:ch-silh01}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=9.5cm]{images/ari-plot-455-2_260-800-hierarchical.png}
    \caption{Hierarchical clustering. Adjusted Rand index for the
    manually annotated set of 455 publications clustered with hierarchical
    clustering, LSA with 800 components. The higher values denote 
    more compact and better clustering. The index gains its peak value at xxx.}
    \label{fig:ari01}
  \end{center}
\end{figure}


In figure \ref{fig:ch-silh02}
we see corresponding graphs for k-means clustering. Top terms for
Calinski-Harabasz index maximum at seven clusters are listed in 
the Table \ref{table:topterms}.


\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=9.5cm]{images/c-h-silh-index-plot-519-2_260-800-kmeans.png}
    \caption{k-means clustering. Calinski-Harabasz index and Silhoutte values for the
    manually annotated set of 455 publications clustered with k-means 
    clustering, LSA with 800 components. The higher values denote 
    more compact and better clustering in both graphs.}
    \label{fig:ch-silh02}
  \end{center}
\end{figure}

\input{tables/5_table_topterms.tex}


\subsection{Precision and recall}
With manually annotated dataset of 455 articles we compared 
the results with three clusters which was assumed the correct 
classification of the data set. We got recall $R = ???$ and 
precision $P = ???$ which is
a quite poor result. By looking at the top terms and random samples
from each cluster we notice that two of the three clusters have 
\emph{'Clinical neurology'} related items, see Tables
\ref{table:topterms_455_hier} and \ref{table:articles_455_hier}.

\input{tables/5_table_topterms_bl.tex}

\input{tables/5_table_articles_bl.tex}


\subsection{Other metrics}
Different metrics to evaluate clustering include (from 
scikit-learn) Adjusted Rand Index, Mutual Information scores 
(NMI, AMI), homogenity, completness, V-measure, Fowlkes-Mallows 
scores, Silhouette Coefficient, Calinski-Harabaz Index, (from 
sources) 


%5.2 Finnish publications
\section{Finnish publications from year 2000}
We clustered year 2000 data, 10145 publications, with k-means 
clustering with different value of $k$, the number of clusters. 
The Figure \ref{fig:ch-silh02} shows the results.
\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=10cm]{images/c-h-silh-index-plot-y2000-2_260-800-kmeans.png}
    \caption{Calinski-Harabasz index and Silhoutte values of 
10145 items clustered with k-means clustering, LSA with 800 
components. The higher values denote more compact and better 
clustering in both graphs.}
    \label{fig:ch-silh02}
  \end{center}
\end{figure}

Similarly the same data was clustered with agglomerative 
hierarchical Ward's clustering by sampling the number of clusters 
from the values [2-260]. The silhouette and Calinski-Harabasz values
are shown in Figure \ref{}
\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=10cm]{images/c-h-silh-index-plot-y2000-2_260-800-hierarchical.png}
    \caption{Hierarchical clustering. Calinski-Harabasz index and Silhoutte values for the
    Finnish publications from year 2000 clustered with hierarchical
    clustering, LSA with 800 components. The higher values denote 
    more compact and better clustering in both graphs.}
    \label{fig:ch-silh-2000-01}
  \end{center}
\end{figure}
We can see from the values that they prefer different features in
the data. Calinski-Harabasz index sees the fewest number of 
clusters - two - as the most compact and optimal clustering. Also
the form of the curve hints to perhaps power law that is also 
known as Zipf's law in text analysis. \fixme{Check this.}
It should be noticed that the silhouette values are quite low 
in absolute terms considering it's index space [-1,1]. This might 
be in line with the fact that it's best suited for [normally?] 
distributed and compact clusters. But we can also see that the 
silhouette values grow with the number of clusters although no
maxima seems to be reached with these cluster values.



% TODO: Kirjoita nämä oikeaan kohtaan tai poista
% Silhoutte values for 6000 publications clustered with 
% agglomerative clustering with Ward's method, number of clusters 
% 64, are seen in Figure~\ref{fig:silh01}.
% \begin{figure}[ht]
%   \begin{center}    
% \includegraphics[width=13cm]{images/6000-64-400-Hierarchical-silhouette-plot.png}
%     \caption{Silhoutte values of 6000 items clustered with 
%     agglomerative clustering with Wrad's method, 64 clusters, 
%     LSA with 400 components. Each pixel row corresponds to a 
%     Silhouette value of an item, adjacent rows separated by gap 
%     corresponds to a cluster.
%     Dashed line is the average.}
%     \label{fig:silh01}
%   \end{center}
% \end{figure}
% 
% Other measure is Calinski-Harabaz index. Clustering should be 
% optimal when Calinski-Harabaz index reaches its maximum
% value~\cite{}.
% 
% When evaluating the results, Silhouette Coefficient might not be
% the best option. Find out why. V-measure or Adjusted Rand Index
% might be better options~\cite{noauthor_clustering_nodate}.






