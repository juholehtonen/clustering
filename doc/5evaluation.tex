\chapter{Results}
\label{chapter:results}

%5.0 Yleistä tuloksista
We first calculated internal and external validation metrics for 
the manually annotated subset of data to see how well our metrics 
revealed our defined ground truth clustering. Based on this we tried 
to evaluate the most suitable internal validation metric to be used 
with the complete data set. This validation metric would suggest us
the optimal number of clusters $k$ to be used in the clustering.
% Finally we inspect the resulting clustering
% and calculated internal validation metrics for it. 
% Finally we compare the clustering with the initial WOS subject 
% categories. [ei aikaa tähän]

%5.1 Manually annotated data  --> the chosen metrics
%5.3 Chosen number of clusters based on metrics (oikeasti tiedetään: 3)
%5.4 Klusterointi kolmeen
%5.5 Klustereiden havainnollistaminen
\section{Manually annotated data}
We evaluated our internal clustering validation metrics by 
creating a smaller subset of publications with manually
checked field of science. This subset consisted of 
$455$ publications from three different fields:
\emph{Computer science: Information systems}, $134$ publications,
\emph{Computer science: Artificial intelligence}, $127$ publications
and \emph{Clinical neurology}, $194$ publications.
The term frequency cut-off values $min_df=2$ (number of occurences) 
and $max_df=0.5$ (portion of documents with the term) resulted
in $4455$ ignored terms and left the feature space of $3177$ terms. 
Terms that were filtered out as too frequent or too rare were for 
example: 
``haemoglobin\_a'', ``jacobian\_matrix'', 
``parthenogenetic'', ``chelation\_by\_saccharide\_molecules'', 
``leg\_blood\_supply'', ``computational\_fluid\_dynamics'', 
``pigment'', ``hdtv'', ``nicotinic\_receptor''. 
% Add line break due to random layout glitch
\linebreak
(Note: keywords were
combined with '\_' as described in Section \ref{sec:impl_preproc}.)
% \fixme{Then we calculated precision and recall for the data set.} 

The dimensionality of data was then reduced with LSA from $3177$
features (corresponding to the terms) to $800$ components which 
resulted $100\%$ of the explained variance of the data to be 
retained. The data was then agglomeratively clustered with Ward's 
method for number of cluster values $k=[2,12]$. 
The Figure \ref{fig:ch-silh01} shows the results. 
\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=11.5cm]{images/c-h-silh-index-plot-455-2_12-800-hierarchical.png}
    \caption{Hierarchical clustering. Calinski-Harabasz index and Silhoutte values for the
    manually annotated set of 455 publications clustered with hierarchical
    clustering, LSA with 800 components. The higher value means
    more compact and better clustering in both graphs.}
    \label{fig:ch-silh01}
    \end{center}
\end{figure}
We see that Calinski-Harabasz index reaches its maximum value 
with the number of clusters at two and that silhouette values 
increases with the number of clusters.
Neither of these indices suggests three clusters as the optimal 
clustering solution. So according these specific metrics the data 
doesn't support our initial idea of three ``natural'' clusters as
we meant it. We repeated the clustering in case of ambiguousness
in algorithm but that didn't affect the clustering result.
Probably the low amount of samples compared to 
still quite high dimensional feature space affects here. We can 
conclude from the explained variance of $100\%$ that dimensionality
could have been reduced more. We also 
have to remember that Calinski-Harabasz index might suffer even 
from moderate noise \cite{liu_understanding_2010}.
% silhouette values (pros, cons...)

% reference: statistical significance samples relation dimension
% Perustelu ARI:n käyttämiseksi: eka katsottiin S ja C-H:lla josko
% toinen osuu k=3:een. Sitten katsottaan myös ARI:ll miten vahvasti
% k=3 erottuu muista k:n arvoista. Ei erotu juurikaan. Mitä tämä
% kertoo tai olisi kertonut?
% Jos olisi erottunut ja S tai C-H olisi erottunut: ok, yhteneväiset todistukset
% Jos olisi erottunut ja S tai C-H ei olisi: klusterointi on siellä mutta S ja C-H oleellisesti huonommat sen ilmaisemiseen (kenties klusterien rakenteen takia)
% Jos ei erotu ja S ja C-H erottaa: ei liene mahdollinen tilanne (?!) (ARI:n määritelmä?)
% Jos ei erotu ja S ja C-H ei erota: haluttu ja viereiset klusteroinnit yhtä epäselviä, data ei riitä/sovellu, tilanne nyt
We then use Adjusted Rand Index to check the quality of resulting 
clustering compared to ground truth. ARI tells us the similarity 
of the defined ground truth clustering and our resulting clustering. 
Figure \ref{fig:ari01} shows ARI values for different number of 
clusters $k=[2,12]$. Because ARI uses the ground truth cluster 
labels for the publications, it peaks at number of clusters at 
$3$. But we see that its value for $k=[4,6]$ is almost equally 
good. We interpret this to mean that each of these clusterings 
with $k=[3,6]$ have approximately equal amount of misclustered 
publications, so our clustering method can't actually find above 
others clustering for $k=3$ from this data.
\begin{figure}[htp]
  \begin{center}    
\includegraphics[width=7.5cm]{images/ari-plot-455-2_12-800-hierarchical.png}
    \caption{Hierarchical clustering. Adjusted Rand index for the
    manually annotated set of 455 publications clustered with hierarchical
    clustering, LSA with 800 components. The higher value means
    more compact and better clustering. The index gains its peak value with 
    number of clusters at $3$.}
    \label{fig:ari01}
  \end{center}
\end{figure}

We inspected the resulting clustering for three clusters by 
looking at the most frequent terms of the publications in each 
cluster. Top 15 terms for each cluster are shown in the Table 
\ref{table:topterms_455_hier}. We can see that there is one cluster
with computer science related terms and two clusters with clinical
neurology related terms. So we can notice that our clustering method
didn't find the clusters we expected. This is also visible in 
adjusted Rand index with the number of clusters $k=[3,6]$ almost 
square (Figure \ref{fig:ari01}). For three clusters, part of the 
samples are incorrectly clustered, when compared against manual
labeling, and for clusterings with four to six clusters, the 
situation is obviously the same.

\input{tables/5_table_topterms_bl.tex}

For a comparison we also tried clustering manually annotated data
with K-means clustering. The idea was to see if this method would
give clearer clusters where the clustering that either silhouette values or 
Calinski-Harabasz index would recognize. Its similar
index values for different number of clusters can be seen in 
figure \ref{fig:ch-silh02}. K-means wasn't able to significantly 
improve the clustering.

\begin{figure}[htp]
  \begin{center}    
\includegraphics[width=11.5cm]{images/c-h-silh-index-plot-455-2_12-800-kmeans.png}
    \caption{K-means clustering. Calinski-Harabasz index and Silhoutte values for the
    manually annotated set of 455 publications clustered with K-means 
    clustering, LSA with 800 components. The higher value means
    more compact and better clustering in both graphs.}
    \label{fig:ch-silh02}
  \end{center}
\end{figure}

We can see that neither of our internal validation method for 
clustering performance doesn't really tell us much. 
% From Table \ref{} we can see the Calinski-Harabasz index values.
We don't get very strong support for using any of these to measures
to decide the number of clusters for the actual one year data 
clustering.
% Top terms for Calinski-Harabasz index maximum at seven clusters are listed in 
% the Table \ref{table:topterms}.


%5.2 Metrics evaluation, also compare to precision and recall??
% \subsection{Precision and recall}
% With manually annotated dataset of 455 articles we compared 
% the results with three clusters which was assumed the correct 
% classification of the data set. We got recall $R = ???$ and 
% precision $P = ???$ which is
% a quite poor result. By looking at the top terms and random samples
% from each cluster we notice that two of the three clusters have 
% \emph{'Clinical neurology'} related items, see Tables
% \ref{table:topterms_455_hier} and \ref{table:articles_455_hier}.

% \input{tables/5_table_topterms_bl.tex}
% \input{tables/5_table_articles_bl.tex}

% \subsection{Other metrics}
% Different metrics to evaluate clustering include (from 
% scikit-learn) Adjusted Rand Index, Mutual Information scores 
% (NMI, AMI), homogenity, completness, V-measure, Fowlkes-Mallows 
% scores, Silhouette Coefficient, Calinski-Harabaz Index, (from 
% sources) 


%5.2 Finnish publications
\section{Finnish publications from year 2000}
We clustered the year 2000 data, 10456 publications, with 
agglomerative clustering using Ward's method with sampling some values
for number of clusters between $[2,260]$ 
% $\{2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 26, 33, 40, 47, 54, 61, 
% 68, 75, 82, 89, 96, 103, 110, 117, 124, 131, 138, 145, 152, 159,
% 166, 173, 180, 187, 194, 201, 208, 215, 222, 229, 236, 243, 250,
% 251, 254, 257, 260\}$
The silhouette and Calinski-Harabasz values are shown in Figure 
\ref{fig:ch-silh-2000-h}.
\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=10cm]{images/c-h-silh-index-plot-y2000-2_260-800-hierarchical.png}
    \caption{Hierarchical clustering. Calinski-Harabasz index and Silhoutte values for the
    Finnish publications from year 2000 clustered with hierarchical
    clustering, LSA with 800 components. The higher values denote 
    more compact and better clustering in both graphs.}
    \label{fig:ch-silh-2000-h}
  \end{center}
\end{figure}
We notice that the graphs are similar to the those of manually
annotated data. Again Calinski-Harabasz index suggests that 
optimal clustering is reached with just two clusters while 
silhouette values tells us that clusterings improve with increasing
number of clusters. Because these cluster validation measures are
in conflict we choose a compromise for the number of clusters: $188$

The top terms of ten random clusters sampled from $188$ are listed
in Table \ref{table:topterms-188-h-short}.
\begin{table}[ht]
  \begin{center}
    \input{tables/5_table_topterms_188_h_short.tex}
    \caption{Top terms for ten random clusters from total 188}
    \label{table:topterms-188-h-short}    
 \end{center}
\end{table}
We note that at least these ten clusters seem resonable concluded
from the top terms. The Table \ref{table:articles-188-h-mini} 
shows five random publication titles and their WoS subject 
categories for three first clusters of Table \ref{table:topterms-188-h-short}. 
See Appendix \ref{chapter:appendix-articles} for sample titles for
all ten example clusters.
\newpage
\input{tables/5_table_articles_188_h_mini.tex}


The Figure \ref{fig:ch-silh-full-h} shows the results.
\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=10cm]{images/c-h-silh-index-plot-y2000-2_260-800-kmeans.png}
    \caption{Calinski-Harabasz index and Silhoutte values of 
10145 items clustered with k-means clustering, LSA with 800 
components. The higher values denote more compact and better 
clustering in both graphs.}
    \label{fig:ch-silh-full-h}
  \end{center}
\end{figure}

We can see from the values that they prefer different features in
the data. Calinski-Harabasz index sees the fewest number of 
clusters - two - as the most compact and optimal clustering. 
% Also the form of the curve hints to perhaps power law that is also 
% known as Zipf's law in text analysis. \fixme{Check this.}
It should be noticed that the silhouette values are quite low 
in absolute terms considering it's index space [-1,1]. This might 
be in line with the fact that it's best suited for [normally?] 
distributed and compact clusters. But we can also see that the 
silhouette values grow with the number of clusters although no
maximum seems to be reached with these cluster values.


% \input{tables/5_table_topterms.tex}

% \input{tables/5_table_topterms_30_h.tex}
























% TODO: Kirjoita nämä oikeaan kohtaan tai poista
% Silhoutte values for 6000 publications clustered with 
% agglomerative clustering with Ward's method, number of clusters 
% 64, are seen in Figure~\ref{fig:silh01}.
% \begin{figure}[ht]
%   \begin{center}    
% \includegraphics[width=13cm]{images/6000-64-400-Hierarchical-silhouette-plot.png}
%     \caption{Silhoutte values of 6000 items clustered with 
%     agglomerative clustering with Wrad's method, 64 clusters, 
%     LSA with 400 components. Each pixel row corresponds to a 
%     Silhouette value of an item, adjacent rows separated by gap 
%     corresponds to a cluster.
%     Dashed line is the average.}
%     \label{fig:silh01}
%   \end{center}
% \end{figure}
