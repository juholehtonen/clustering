\chapter{Results}
\label{chapter:results}

%5.0 Yleistä tuloksista
We first calculated internal and external validation metrics for 
the manually annotated subset of data to see how well our metrics 
revealed our defined ground truth clustering. Based on metrics we 
chose the number of clusters $n$ as our solution to the problem
and clustered the whole data for one year.
% Finally we inspect the resulting clustering
% and calculated internal validation metrics for it. 
% Finally we compare the clustering with the initial WOS subject 
% categories. [ei aikaa tähän]

%5.1 Manually annotated data  --> the chosen metrics
%5.2 Metrics evaluation, also compare to precision and recall??
%5.3 Chosen number of clusters based on metrics (oikeasti tiedetään: 3)
%5.4 Klusterointi kolmeen
%5.5 Klustereiden havainnollistaminen
\section{Manually annotated data}
% TODO: tämä selostettu jo 3-luvussa?
We evaluated our clustering by creating an evaluation set with
hand labeled fields of science. This subset of data consisted of 
$455$ publications from three different fields. The fields were:
\emph{Computer science: Information systems}, $134$ publications,
\emph{Computer science: Artificial intelligence} $127$ publications, 
and \emph{Clinical neurology} $194$ publications as classified by 
publisher. 
% So, our manual classification will be subjective but gives some 
% hint about the clustering performance.
% \fixme{Then we calculated precision and recall for the data set.} 

The Figure \ref{fig:ch-silh01} shows the results. We see that
Calinski-Harabasz index reaches it's maximum value at the number
of clusters two and that silhouette values increases with the
number of clusters.
Neither of these indices corresponds to the three disciplines
that we selected manually. It's also worth to remember that there
is only 455 data points to cluster. Comparison against preferred
manual labeling is calculated with the 
Adjusted Rand Index (ARI) for these clusterings in Figure 
\ref{fig:ari01}. Because ARI uses manually annotated labels, the 
groundtruth values for the clusters, it peaks at $3$ as the number 
of clusters with $4$ as almost equally good value.

\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=9.5cm]{images/c-h-silh-index-plot-519-2_260-800-hierarchical.png}
    \caption{Hierarchical clustering. Calinski-Harabasz index and Silhoutte values for the
    manually annotated set of 455 publications clustered with hierarchical
    clustering, LSA with 800 components. The higher values denote 
    more compact and better clustering in both graphs.}
    \label{fig:ch-silh01}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=9.5cm]{images/ari-plot-455-2_260-800-hierarchical.png}
    \caption{Hierarchical clustering. Adjusted Rand index for the
    manually annotated set of 455 publications clustered with hierarchical
    clustering, LSA with 800 components. The higher values denote 
    more compact and better clustering. The index gains its peak value at $3$.}
    \label{fig:ari01}
  \end{center}
\end{figure}

Top terms for maximum index values are shown in the Table 
\ref{table:topterms_455_hier}. We can see that there are one cluster
with computer science related terms and two clusters with clinical
neurology related terms. So we can note that our clustering method
didn't find the clusters that we expected. This may be visible also
in adjusted rand index with cluster amounts three and four almost
square. For three clusters part of the samples are incorrectly 
clustered and for four clusters the situation is obviously the same.

\input{tables/5_table_topterms_bl.tex}

For a comparison we also tried clustering manually annotated data
with k-means clustering. It's practically identical index values 
for different number of clusters can be seen in figure \ref{fig:ch-silh02}.

% Top terms for Calinski-Harabasz index maximum at seven clusters are listed in 
% the Table \ref{table:topterms}.

\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=9.5cm]{images/c-h-silh-index-plot-519-2_260-800-kmeans.png}
    \caption{k-means clustering. Calinski-Harabasz index and Silhoutte values for the
    manually annotated set of 455 publications clustered with k-means 
    clustering, LSA with 800 components. The higher values denote 
    more compact and better clustering in both graphs.}
    \label{fig:ch-silh02}
  \end{center}
\end{figure}

% \input{tables/5_table_topterms.tex}

% \input{tables/5_table_topterms_30_h.tex}


\subsection{Precision and recall}
With manually annotated dataset of 455 articles we compared 
the results with three clusters which was assumed the correct 
classification of the data set. We got recall $R = ???$ and 
precision $P = ???$ which is
a quite poor result. By looking at the top terms and random samples
from each cluster we notice that two of the three clusters have 
\emph{'Clinical neurology'} related items, see Tables
\ref{table:topterms_455_hier} and \ref{table:articles_455_hier}.

% \input{tables/5_table_topterms_bl.tex}

% \input{tables/5_table_articles_bl.tex}


% \subsection{Other metrics}
% Different metrics to evaluate clustering include (from 
% scikit-learn) Adjusted Rand Index, Mutual Information scores 
% (NMI, AMI), homogenity, completness, V-measure, Fowlkes-Mallows 
% scores, Silhouette Coefficient, Calinski-Harabaz Index, (from 
% sources) 


%5.2 Finnish publications
\section{Finnish publications from year 2000}
We clustered year 2000 data, 10145 publications, with k-means 
clustering with different value of $k$, the number of clusters. 
The Figure \ref{fig:ch-silh-full-h} shows the results.
\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=10cm]{images/c-h-silh-index-plot-y2000-2_260-800-kmeans.png}
    \caption{Calinski-Harabasz index and Silhoutte values of 
10145 items clustered with k-means clustering, LSA with 800 
components. The higher values denote more compact and better 
clustering in both graphs.}
    \label{fig:ch-silh-full-h}
  \end{center}
\end{figure}

Similarly the same data was clustered with agglomerative 
hierarchical Ward's clustering by sampling the number of clusters 
from the values [2-260]. The silhouette and Calinski-Harabasz values
are shown in Figure \ref{}
\begin{figure}[ht]
  \begin{center}    
\includegraphics[width=10cm]{images/c-h-silh-index-plot-y2000-2_260-800-hierarchical.png}
    \caption{Hierarchical clustering. Calinski-Harabasz index and Silhoutte values for the
    Finnish publications from year 2000 clustered with hierarchical
    clustering, LSA with 800 components. The higher values denote 
    more compact and better clustering in both graphs.}
    \label{fig:ch-silh-2000-01}
  \end{center}
\end{figure}
We can see from the values that they prefer different features in
the data. Calinski-Harabasz index sees the fewest number of 
clusters - two - as the most compact and optimal clustering. 
% Also the form of the curve hints to perhaps power law that is also 
% known as Zipf's law in text analysis. \fixme{Check this.}
It should be noticed that the silhouette values are quite low 
in absolute terms considering it's index space [-1,1]. This might 
be in line with the fact that it's best suited for [normally?] 
distributed and compact clusters. But we can also see that the 
silhouette values grow with the number of clusters although no
maximum seems to be reached with these cluster values.



% TODO: Kirjoita nämä oikeaan kohtaan tai poista
% Silhoutte values for 6000 publications clustered with 
% agglomerative clustering with Ward's method, number of clusters 
% 64, are seen in Figure~\ref{fig:silh01}.
% \begin{figure}[ht]
%   \begin{center}    
% \includegraphics[width=13cm]{images/6000-64-400-Hierarchical-silhouette-plot.png}
%     \caption{Silhoutte values of 6000 items clustered with 
%     agglomerative clustering with Wrad's method, 64 clusters, 
%     LSA with 400 components. Each pixel row corresponds to a 
%     Silhouette value of an item, adjacent rows separated by gap 
%     corresponds to a cluster.
%     Dashed line is the average.}
%     \label{fig:silh01}
%   \end{center}
% \end{figure}
% 
% Other measure is Calinski-Harabaz index. Clustering should be 
% optimal when Calinski-Harabaz index reaches its maximum
% value~\cite{}.
% 
% When evaluating the results, Silhouette Coefficient might not be
% the best option. Find out why. V-measure or Adjusted Rand Index
% might be better options~\cite{noauthor_clustering_nodate}.






