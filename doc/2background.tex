\chapter{Background}
\label{chapter:background}

% Tiedonhankintaa suunnitellessasi voi miettiä vastauksia mm. 
% seuraaviin aiheen määrittelyä selventäviin tutkimuskysymyksiin:
% 
%     Mistä aiheesta tietoa tarvitaan?
%       bibliometriikasta, sen määritelmästä sekä klusteroinnista
%     Mihin tarkoitukseen tietoa tarvitaan?
%       Aiheen taustan kuvailuun, menetelmien kuvailuun ja 
%       valitun menetelmän toteuttamiseen
%     Mikä aiheessa on keskeistä?
%       Klusterointimenetelmän kokeilu ja tulosten raportointi
%     Mistä näkökulmasta aihetta lähestytään?
%       Käytännön implementaation kokeilulla
%     Mitä aiheesta tiedetään jo ennalta?
%       Klusteroinnista perusteet, bibliometriikasta vähemmän
%     Tarvitaanko yleis- vai tieteellistä tietoa?
%       Bibliometriikasta tarvitaan vähän yleistietoa, 
%       klusteroinnista tieteellistä.
%     Tarvitaanko kuva-aineistoa?
%       Ei muuta kuin itse tuotetut
%     Minkä ikäistä tietoa tarvitaan?
%       Yleis- ja taustatiedot vanhoista asioista, aiheen 
%       oleellinen tieto uusinta
%     Minkä kielistä tietoa tarvitaan?
%       suomi ja englanti käy


In this chapter we briefly introduce clustering and how it is 
positioned in the larger field of machine learning. But first we
describe what bibliometrics is.
% Taman esittelyjarjestyksen voi vaihtaa myohemmin

\section{Bibliometrics}
\label{sec:bibliometrics}
% Mita bibliometriikka on?
% Valmis --->-v
Bibliometrics is a study of written scientific records. The 
records may be books, articles, letters in scientific journals, 
conference papers and so on. Bibliometrics studies how these 
products of research are communicated, how are they related to 
each other, what kind of properties they have and what can be 
learned about the science in general by analysing them.

% Mihin bibliometriikka pyrkii vastaamaan?
Bibliometrics seeks to answer questions like ``How many 
publications has an author authored?'', ``How many citations an 
author has'', ``What are the 
cited publications of a scientific document?''. It also studies
a bit more boarder questions like ``How many publications
on discipline X been published a year?'', ``Which research area
does this publication belong to?'', ``What other publications 
belong to this research area?'', ``When has this research area
emerged?'', ``What are the most important related research areas
of this discipline?'' an so on.

% Background/history
% ==================
% Mika bibliometriikan historia on?
% OK --->-v
Classifying things is often the first thing we do when we observe 
the world. On the other hand, counting the frequenzies of objects
and comparing these numbers often helps to put things in 
perspective.
% Tahan voisi laittaa sidontaa todennakoisyyslaskennalla tjsp.
One of the earliest studies that is generally considered
bibliometrics was Cole's and Eales' analysis to the anatomy 
literature in 1917 \cite{cole_history_1917}. In this study they 
researched the anatomy literature from 1543 through 1860 with the 
intention to graph the growth of of the number of documents over 
the three centuries, to present ``the performance'' of each 
European country, to observe the most popular topics among 
scholars from time to time, and to compare the advancement and 
devolution stages of the research with different societal 
events \cite{bellis_bibliometrics_2009}.
As the number of scientific publications has enormously increased
the need to automatically analyze them has become apparent.
The basic analysis on top of which more detailed studies can be 
built on is classifying each publication to research areas and
disciplines.
% The need for some kind of bibliometric indices rise in the 
% First modern(?) classification was... by... some 
% indexing/publishing/to facilitate communication...
% At some point more and more automation was needed for bibliomterics
% There has been lot of study in automatically classificating the 
% science. 


\subsection{Classification in bibliometrics}
% Specific charasteristics of classifying the bibliometrics
% Miten bibliometriikkaa voidaan jasentaa / mista se koostuu?
% Existing classification systems / methods
% =========================================
% \fixme{Onko Scopus tässä relevantti jos ei Scopus-dataa?}
Currently, the most popular systems to classify the publications 
into research areas are the Clarivate's (formerly Thomson Reuters)
Web of Science and Elsevier's Scopus classification
systems. These classification systems classify the journals into
one or more research areas \cite{waltman_new_2012}. Publications
are then classified to research areas based on in which journal they
were published. WoS uses approximately 250 \emph{subject categories}
in its classification. Each journal can belong 
to one or up to six categories. The categories have been created
at least or before early 1960's by manually classifying journals.
New journals were added one at the time after visual inspection of
citation information. New categories were added when when old ones
grew \cite{pudovkin_algorithmic_2002}. As for Scopus, according to 
Wang and Waltman \cite{wang_large-scale_2016}, ``there seems to be no 
information at all on the construction of its classification 
system''. We will use the data from WoS database in this work.

Also an independent journal level classification system has been 
developed. \cite{archambault_towards_2011}
Journal level classification systems have known limitations.
They are, for example, unable to meaningfully classify
publications published in multidiscipline journals.
Also some discipline specific classification systems exists such 
as (check them...).
An alternative classification system is publication level 
classification where each publication is classified based on some 
information extracted directly from the publication self such as
words used in the title and/or abstract, keywords attached by the
authors or publisher or citations to other publications.
Shu et al. have compared journal and paper level classification
approaches and found that publication level classification could
provide better classification \cite{shu_comparing_2019}.
% Usually classification of publications or journals can be approached 
% at least from multiple directions. There is clustering based method,
% the network based method and the combination of the two.
% In the network method

% ACM:n luokittelujärjestelmä esimerkkinä? CLSF?...
Bibliometric research 
uses mainly three types of methods; citation based, text analysis 
based or combination of the two \cite{janssens_hybrid_2009}.
Citation based methods study citations of publications and produce
networks where publications are nodes connected by citations as 
edges. In
 the rare case of publication having a direct quote including a
 citation, the citation is not counted unless it is also a
 citation of the publication itself.
Connection between two publications can be 
formed by a direct citation, bibliometric coupling where 
publications are connected when a third publication cites them 
both or co-citations where publications are connected if they 
cite the same third publication.

Text analysis based methods examine the title, the abstract or 
the whole text content of the publication itself and classify 
the publications or journals by the topic model created 
\cite{blei_latent_2003}.
Hybrid models combine both approaches. Authors and their
affilations are not used in this work because we cannot uniquely
identify them and we can't assume their field of science that is
our interest here.

% Tama on kompelosti ilmaistu, korjaa
% Verkkoanalyysin nakokulma
% These research products and relations between them form a 
% network that can reveal something about the structure of 
% different scientific disciplines. Finding the structure of this 
% kind of data set is called classification or clustering.


% Previous results using clustering bibliometrics
% ===============================================
% Kirjoitettu yllä olevaan


\subsection{Bibliometrics in Finland}
Ministry of Education and Culture of Finland provides yearly
updated bibliometric analyses of Finnish research activities 
based on both Web Of Science citation index 
and Elsevier's Scopus database \cite[Vipunen 
service]{noauthor_ministry_2019}. The corresponding source 
system classification for a field of science is used and
aggregated to match the Statistics Finland classification 
\cite{auranen_tieteen_2018}. CSC - 
IT Center for Science is responsible of the actual technical 
implementation of the service.

One of the earlier and comprehensive bibliometric research of 
Finnish science is a report by Persson et al. 
\cite{persson_bibliometric_2000} which mapped the situation and 
development of Finnish science 1981-1998.
This, however, is a report which concentrates on bibliometric 
analysis based on the map of science provided by WoS subject 
categories. But if we want to explore how the Finnish scientific 
disciplines themselves have evolved over time these pre-defined 
subject categories are quite rigid framework. For that reason we 
want to experiment creating an alternative mapping of science by 
clustering. 

So, as opposed to bibliometric analyses seeking to understand the
state of a scientific discipline as defined by some existing 
definition, we want to experiment/inspect how to define 
scientific disciplines to be used in bibliometric analyses.

%Efforts to cluster Finnish research include a study by...

Suominen and Toivanen used unsupervised learning-based topic 
modeling to create a map of science for Finnish publications from 
1995-2011. They evaluated it by comparing the results with WoS 
based classification and concluded that superiority of the method
depends on the purpose of analysis. Traditional manually created 
classifications are relevant for information retrieval while 
machine learning methods can reveal new emerging areas of science 
\cite{suominen_map_2016}. Compared to our work here, their 
analysis method topic modeling differs from hierarchical 
clustering albeit both are unsupervised learning methods.
We will return to compare our results to those of Suominen and 
Toivanen in Discussion chapter.

Our research question here is: "How to automatically cluster 
Finnish scientific publications and how does that clustering 
compare to existing fiels of science classification by WoS?" We 
will use hierarchical clustering on features derived from titles,
abstracts and keywords in publication metadata.


\section{Clustering}
The methods discussed in this work belong to the field of machine 
learnig. The field has it's roots in statistics and engineering 
and is itself part of artificial intelligence.
The methods in machine learnig can be divided to supervised, 
unsupervised and reinforced learning \cite{alpaydin2004introduction}.

Commonly for all methods we define $X$ as
the sample data and $Y$ as label indicating which class our data 
sample $X$ belongs. We also have to choose the model $f()$ for
learning from the data. Then we can state our learning problem as
a function $Y = f(w \cdot X)$, where $w$ is a weight vector, 
which would give a prediction of the class $Y$ of the sample data
$X$. 
Assuming we have enough of samples $X$ we then teach our model 
with training data. That is, we solve the weights $w$ using the
loss function such that it optimizes the difference between the 
true and predicted class labesl $Y$.

Supervised learnig methods include classification and continuous 
estimation (regression)
where the class of the training samples, or the target values in 
case of regression, $Y$ is known. Example of a classification 
problem is optical character recognition where the system is 
taught with example of characters along with their correct labels.

For unsupervised learning the correct answer or the label $Y$ is not known. 
Instead a model is applied such that it finds regularities in the
input data $X$. Example of unsupervised learning problem is 
finding anomalities that don't fit in the group, such as analysing
log files of a computer system to find a possible intruder.

Unsupervised learning methods include clustering, dimensionality
reduction and topic modeling for instance. Clustering try to 
distinguish patterns in the data and discriminate unrelated 
objects into separate clusters and aggregate related objects into
same cluster.

In reinforced learning we want the system to learn a sequence of
actions leading to desired outcome. For example we may want the 
system to learn to win a game. In that case individual actions
are not important but the end result as there are many ways to win
a game. So the system repaeatedly tries different combinations of
actions while it receives the result of it's combined actions.

\subsection{Model selection}
Generally all machine learning problems are ill-posed in the sense 
that a unique solution for the problem can't be found unless some
assumptions, or \emph{inductive bias}, are introduced. This begins 
with selecting the learning algorithm and might also include some
hyperparameters of selected algorithm.

Here we will use unsupervised learning to shape the mapping of 
scientific disciplines because we want to learn the possible 
intristic structure of sciencetific knowledge.  
There are many different clustering algorithms from which to 
choose. Some often used common algorithms are k-means, hierarchical
clustering, density based scan clustering and Gaussian clustering.
We will use hierarchical clustering because it's quite simple and 
familiar to us.
In hierarchical clustering there are yet different parameters to 
choose. 
% We will handle those in later chapters.
Hierarchical clustering with single, average and complete linkages
and Ward's method applied to search query result clustering were 
studied by Korenius et al. \cite{korenius_hierarchical_2006}.

\subsection{Choosing the number of clusters}
The number of clusters $k$ that should result from the clustering is
not known beforehand but is required a parameter of the
clustering algorithm. Hierarchical clustering will always return 
$k$ clusters regardless if they are meaningful or not. To make a
decision about the number of clusters we can 1) inspect the data in 
two dimensions using some dimensionality reduction method such as 
PCA and then plot it, 2) set limits for the inter-cluster 
distances at each step of merging clusters ie. if finding the 
largest gap between the dendrogram levels or 3) measuring the 
overall compactness of clusters and their separation from each 
other over the number clusters $k$ with suitable evaluation 
criterion such as average Silhouette value 
\cite{alpaydin2004introduction} \cite{calinski_dendrite_1974}(?) 
\cite{rousseeuw_silhouettes:_1987}. The first method is assumed to 
give uninformative view of the data. We will use the third method.
% Toinen vaihtoehto mahdollisesti samankaltainen kolmannen kanssa?


\subsubsection{Internal validation of clustering results}
Liu et al. have reviewed 11 commmonly used internal clustering 
validation indices \cite{liu_understanding_2010}. These are used 
to decide the correct number of clusters. We will use two 
of those; Calinski-Harabasz criterion \cite{calinski_dendrite_1974} 
and Silhouette value \cite{rousseeuw_silhouettes:_1987}. We choose 
these because they were familiar for us and have different
weak spots as noted by Liu et al and thus could complement each 
other. Calinski-Harabasz criterion is defined as
\begin{equation}
 CH = \frac{SS_B}{N-k} \frac{SS_W}{k-1},
\end{equation}
where $SS_B$ is the between-group sum of squares, $SS_W$ is the
within-group sum of squares, $k$ is the number of clusters 
and $N$ is the number of observations.

The between-group sum of squares $SS_B$ gives the overall variance
between clusters
\begin{equation}
 SS_B = \sum_{i=1}^k n_i ||m_i-m||^2,
\end{equation}
where $k$ is the number of clusters, $n_i$ is the number of 
observations in cluster $i$, $m_i$ is the centroid of cluster $i$, 
$m$ is the overall mean of sample data and $||m_i-m||$ is the 
Euclidean distance between the two vectors.

The within-group sum of squares $SS_W$ gives the over variance 
within clusters
\begin{equation}
 SS_W = \sum_{i=1}^k \sum_{x\in c_i} ||x-m_i||^2,
\end{equation}
where $k$ is the number of clusters, $x$ is the observation, 
$c_i$ is the $i$th cluster, $m_i$ is the centroid of cluster $i$, and 
$||x-m_i||$ is the Euclidean distance between the two vectors.
The larger the Calinski-Harabasz criterion, the better the 
cluster structure.


\subsubsection{External validation of clustering results}
% Otsikko oli: Manually annotated validation set
% Gold standard set. Actually a gold standard set would be a set
% of all data sets with abstract excluding sets that don't have it.
We will create a manually annotated validation set for calculating
precision, recall and metrics derived from those.
% määrittele peruskäsitteet kuten precision ja recall
The validation
set consists of $500$ publications from three different fields of
science, two more similar sub fields of computer science, 
information systems and artificial intelligence, and one more
distant from those, clinical neurology.

Publications are inspected by title, abstract, keywords, journal
and publisher assigned disciplines of the journal. We checked 
just by layman's reasoning if the labeled discipline seemed 
plausible. Because of the journal based 
classification, most publications had more than one 
assigned discipline. Only discipline labels named 
previously were retained because we were wanted test 
how well our clustering method separated these three 
groups regardless their labels. So essentially the 
labels could have been eg. 1, 2, 3. Publications
with critically missing data, unclear discipline assignment and
heavily applied publications were excluded from validation set.
Goal was to achieve evaluation measurements based on a quite 
clearly separated set of publications. More vaguely classifiable
publications were included for comparison. We asked an another 
opinion for publications that could have been in more 
than one of our categories. For manually curated validation set 
with discipline assignment and justifications for
possible exclusion see appendix (Insert reference!).

The basic problem is that fields of science can not be 
defined so that they clearly differ from each other. Where one 
discipline end the other has already started like metallurgy and 
material science. Likewise, there are lots of publications that 
handle topics belonging to more than one discipline, eg. this 
thesis discusses clustering and bibliometrics. So when annotating 
publications, deciding if a publication belonged to
a discipline or not felt often quite difficult. Often the 
separation between disciplines felt quite arbitrary. For example
an article describing using wavelet transformation for coding noisy
images was decided to belong to CS information systems whereas an
article describing wavelet based corner detection using SVD was
decided to belong to CS artificial intelligence.
For CS artificial intelligence we mostly selected publications 
which
mentioned some dimensionalty reduction or machine learning related
term or concept.
CS information systems ended up being quite like some ``others'' 
or
``the rest'' dump class. It would have publications such as
``A reference model for conceptualising the convergence of 
telecommunications and datacommunications service platforms'',
``Developing a distributed meeting service to support mobile 
meeting participants'',
``On voice quality of IP voice over GPRS'',
\fixme{Perustele vain yhteen alaan luokittelu. ``Tukeudun 
WOS-luokitteluun''.}
\fixme{Selvennä yläluokka-alaluokka-jako: CS general vs. CS 
information system tai CS artificial intelligence.}
