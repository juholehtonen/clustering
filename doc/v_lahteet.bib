
@misc{noauthor_clustering_nodate,
	title = {Clustering text documents using k-means — scikit-learn 0.19.0 documentation},
	url = {http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py},
	urldate = {2017-10-12},
	file = {Clustering text documents using k-means — scikit-learn 0.19.0 documentation:/home/jlehtonen/Zotero/storage/RW257QDW/document_clustering.html:text/html}
}

@article{suominen_map_2016,
	title = {Map of science with topic modeling: {Comparison} of unsupervised learning and human-assigned subject classification},
	volume = {67},
	issn = {2330-1643},
	shorttitle = {Map of science with topic modeling},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/asi.23596/abstract},
	doi = {10.1002/asi.23596},
	abstract = {The delineation of coordinates is fundamental for the cartography of science, and accurate and credible classification of scientific knowledge presents a persistent challenge in this regard. We present a map of Finnish science based on unsupervised-learning classification, and discuss the advantages and disadvantages of this approach vis-à-vis those generated by human reasoning. We conclude that from theoretical and practical perspectives there exist several challenges for human reasoning-based classification frameworks of scientific knowledge, as they typically try to fit new-to-the-world knowledge into historical models of scientific knowledge, and cannot easily be deployed for new large-scale data sets. Automated classification schemes, in contrast, generate classification models only from the available text corpus, thereby identifying credibly novel bodies of knowledge. They also lend themselves to versatile large-scale data analysis, and enable a range of Big Data possibilities. However, we also argue that it is neither possible nor fruitful to declare one or another method a superior approach in terms of realism to classify scientific knowledge, and we believe that the merits of each approach are dependent on the practical objectives of analysis.},
	language = {en},
	number = {10},
	urldate = {2017-09-26},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Suominen, Arho and Toivanen, Hannes},
	month = oct,
	year = {2016},
	keywords = {automatic classification, machine learning, text mining, tulostettu},
	pages = {2464--2476},
	file = {Full Text PDF:/home/jlehtonen/Zotero/storage/EARSCJRN/Suominen and Toivanen - 2016 - Map of science with topic modeling Comparison of .pdf:application/pdf;Snapshot:/home/jlehtonen/Zotero/storage/QEVSI2MR/abstract.html:text/html}
}

@article{janssens_hybrid_2008,
	title = {A hybrid mapping of information science},
	volume = {75},
	issn = {0138-9130, 1588-2861},
	url = {https://link-springer-com.libproxy.aalto.fi/article/10.1007/s11192-007-2002-7},
	doi = {10.1007/s11192-007-2002-7},
	abstract = {Previous studies have shown that hybrid clustering methods that incorporate textual content and bibliometric information can outperform clustering methods that use only one of these components. In thi},
	language = {en},
	number = {3},
	urldate = {2017-08-16},
	journal = {Scientometrics},
	author = {Janssens, Frizo and Glänzel, Wolfgang and Moor, Bart De},
	month = jun,
	year = {2008},
	keywords = {tulostettu},
	pages = {607--631},
	file = {Full Text PDF:/home/jlehtonen/Zotero/storage/F74VJN77/Janssens et al. - 2008 - A hybrid mapping of information science.pdf:application/pdf;Snapshot:/home/jlehtonen/Zotero/storage/FAI3UPQT/10.html:text/html}
}

@article{waltman_new_2012,
	title = {A new methodology for constructing a publication-level classification system of science},
	volume = {63},
	copyright = {© 2012 ASIS\&T},
	issn = {1532-2890},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.22748},
	doi = {10.1002/asi.22748},
	abstract = {Classifying journals or publications into research areas is an essential element of many bibliometric analyses. Classification usually takes place at the level of journals, where the Web of Science subject categories are the most popular classification system. However, journal-level classification systems have two important limitations: They offer only a limited amount of detail, and they have difficulties with multidisciplinary journals. To avoid these limitations, we introduce a new methodology for constructing classification systems at the level of individual publications. In the proposed methodology, publications are clustered into research areas based on citation relations. The methodology is able to deal with very large numbers of publications. We present an application in which a classification system is produced that includes almost 10 million publications. Based on an extensive analysis of this classification system, we discuss the strengths and the limitations of the proposed methodology. Important strengths are the transparency and relative simplicity of the methodology and its fairly modest computing and memory requirements. The main limitation of the methodology is its exclusive reliance on direct citation relations between publications. The accuracy of the methodology can probably be increased by also taking into account other types of relations–for instance, based on bibliographic coupling.},
	language = {en},
	number = {12},
	urldate = {2019-04-20},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Waltman, Ludo and Eck, Nees Jan van},
	year = {2012},
	keywords = {bibliometrics, citation analysis, tulostettu},
	pages = {2378--2392},
	file = {Snapshot:/home/jlehtonen/Zotero/storage/N7HTZD55/asi.html:text/html;Submitted Version:/home/jlehtonen/Zotero/storage/GWM3CFA4/Waltman and Eck - 2012 - A new methodology for constructing a publication-l.pdf:application/pdf}
}

@article{janssens_hybrid_2009,
	title = {Hybrid clustering for validation and improvement of subject-classification schemes},
	volume = {45},
	issn = {0306-4573},
	url = {http://www.sciencedirect.com/science/article/pii/S0306457309000673},
	doi = {10.1016/j.ipm.2009.06.003},
	abstract = {A hybrid text/citation-based method is used to cluster journals covered by the Web of Science database in the period 2002–2006. The objective is to use this clustering to validate and, if possible, to improve existing journal-based subject-classification schemes. Cross-citation links are determined on an item-by-paper procedure for individual papers assigned to the corresponding journal. Text mining for the textual component is based on the same principle; textual characteristics of individual papers are attributed to the journals in which they have been published. In a first step, the 22-field subject-classification scheme of the Essential Science Indicators (ESI) is evaluated and visualised. In a second step, the hybrid clustering method is applied to classify the about 8300 journals meeting the selection criteria concerning continuity, size and impact. The hybrid method proves superior to its two components when applied separately. The choice of 22 clusters also allows a direct field-to-cluster comparison, and we substantiate that the science areas resulting from cluster analysis form a more coherent structure than the “intellectual” reference scheme, the ESI subject scheme. Moreover, the textual component of the hybrid method allows labelling the clusters using cognitive characteristics, while the citation component allows visualising the cross-citation graph and determining representative journals suggested by the PageRank algorithm. Finally, the analysis of journal ‘migration’ allows the improvement of existing classification schemes on the basis of the concordance between fields and clusters.},
	number = {6},
	urldate = {2019-04-25},
	journal = {Information Processing \& Management},
	author = {Janssens, Frizo and Zhang, Lin and Moor, Bart De and Glänzel, Wolfgang},
	month = nov,
	year = {2009},
	keywords = {Hybrid clustering, Journal cross-citation, Mapping of science, Subject classification, tulostettu},
	pages = {683--702},
	file = {ScienceDirect Snapshot:/home/jlehtonen/Zotero/storage/QYSXVLRG/S0306457309000673.html:text/html;Submitted Version:/home/jlehtonen/Zotero/storage/8TCV5VPE/Janssens et al. - 2009 - Hybrid clustering for validation and improvement o.pdf:application/pdf}
}

@article{archambault_towards_2011,
	title = {Towards a multilingual, comprehensive and open scientific journal ontology},
	abstract = {This paper describes the development of a new journal ontology to facilitate the production of bibliometric data. A number of approaches have been used to design journal-level taxonomies or ontologies, and the scholarly research and practical application of these systems have revealed their various benefits and limitations. To date, however, no single classification scheme has been widely adopted by the international bibliometric community. In light of these factors, the new classification presented here—featuring a hierarchical, three-level classification tree—was developed based on best-practice taxonomies. Categories were modelled on those of existing journal classifications (ISI, CHI, ERA), and their groupings of journals acted as “seeds” or attractors for journals in the new classification. Individual journals were assigned to single, mutually exclusive categories via a hybrid approach combining algorithmic methods and expert judgment. Notably, the classification was designed to be as inclusive as possible of newer fields of inquiry; general and multidisciplinary journals; and the range of arts and humanities disciplines. The new scientific journal ontology is freely available (it can be found at www.sciencemetrix.com) under a creative commons license and is operational in 18 languages.},
	language = {en},
	journal = {Proceedings of the 13th international conference of the international society for scientometrics and informetrics},
	author = {Archambault, Eric and Beauchesne, Olivier H and Caruso, Julie},
	year = {2011},
	note = {Durban South Africa},
	pages = {66--77},
	file = {Archambault et al. - Towards a multilingual, comprehensive and open sci.pdf:/home/jlehtonen/Zotero/storage/SN3JQ58H/Archambault et al. - Towards a multilingual, comprehensive and open sci.pdf:application/pdf}
}

@article{boyack_clustering_2011,
	title = {Clustering {More} than {Two} {Million} {Biomedical} {Publications}: {Comparing} the {Accuracies} of {Nine} {Text}-{Based} {Similarity} {Approaches}},
	volume = {6},
	issn = {1932-6203},
	shorttitle = {Clustering {More} than {Two} {Million} {Biomedical} {Publications}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0018029},
	doi = {10.1371/journal.pone.0018029},
	abstract = {Background We investigate the accuracy of different similarity approaches for clustering over two million biomedical documents. Clustering large sets of text documents is important for a variety of information needs and applications such as collection management and navigation, summary and analysis. The few comparisons of clustering results from different similarity approaches have focused on small literature sets and have given conflicting results. Our study was designed to seek a robust answer to the question of which similarity approach would generate the most coherent clusters of a biomedical literature set of over two million documents. Methodology We used a corpus of 2.15 million recent (2004-2008) records from MEDLINE, and generated nine different document-document similarity matrices from information extracted from their bibliographic records, including titles, abstracts and subject headings. The nine approaches were comprised of five different analytical techniques with two data sources. The five analytical techniques are cosine similarity using term frequency-inverse document frequency vectors (tf-idf cosine), latent semantic analysis (LSA), topic modeling, and two Poisson-based language models – BM25 and PMRA (PubMed Related Articles). The two data sources were a) MeSH subject headings, and b) words from titles and abstracts. Each similarity matrix was filtered to keep the top-n highest similarities per document and then clustered using a combination of graph layout and average-link clustering. Cluster results from the nine similarity approaches were compared using (1) within-cluster textual coherence based on the Jensen-Shannon divergence, and (2) two concentration measures based on grant-to-article linkages indexed in MEDLINE. Conclusions PubMed's own related article approach (PMRA) generated the most coherent and most concentrated cluster solution of the nine text-based similarity approaches tested, followed closely by the BM25 approach using titles and abstracts. Approaches using only MeSH subject headings were not competitive with those based on titles and abstracts.},
	language = {en},
	number = {3},
	urldate = {2019-06-14},
	journal = {PLOS ONE},
	author = {Boyack, Kevin W. and Newman, David and Duhon, Russell J. and Klavans, Richard and Patek, Michael and Biberstine, Joseph R. and Schijvenaars, Bob and Skupin, André and Ma, Nianli and Börner, Katy},
	month = mar,
	year = {2011},
	keywords = {Algorithms, Clustering algorithms, Language, Neurons, Computer engineering, Information retrieval, Research grants, Semantics, tulostettu},
	pages = {e18029},
	file = {Full Text PDF:/home/jlehtonen/Zotero/storage/FQJC2RZC/Boyack et al. - 2011 - Clustering More than Two Million Biomedical Public.pdf:application/pdf;Snapshot:/home/jlehtonen/Zotero/storage/J5SSSYHB/article.html:text/html}
}

@article{shu_comparing_2019,
	title = {Comparing journal and paper level classifications of science},
	volume = {13},
	issn = {1751-1577},
	url = {http://www.sciencedirect.com/science/article/pii/S1751157718303298},
	doi = {10.1016/j.joi.2018.12.005},
	abstract = {The classification of science into disciplines is at the heart of bibliometric analyses. While most classifications systems are implemented at the journal level, their accuracy has been questioned, and paper-level classifications have been considered by many to be more precise. However, few studies investigated the difference between journal and the paper classification systems. This study addresses this gap by comparing the journal- and paper-level classifications for the same set of papers and journals. This isolates the effects of classification precision (i.e., journal- or paper-level) to reveal the extent of paper misclassification. Results show almost half of papers could be misclassified in journal classification systems. Given their importance in the construction and analysis of bibliometric indicators, more attention should be given to the robustness and accuracy of these disciplinary classifications schemes.},
	number = {1},
	urldate = {2019-06-17},
	journal = {Journal of Informetrics},
	author = {Shu, Fei and Julien, Charles-Antoine and Zhang, Lin and Qiu, Junping and Zhang, Jing and Larivière, Vincent},
	month = feb,
	year = {2019},
	keywords = {Chinese Library Classification, Classification system of science, CSCD, tulostettu},
	pages = {202--225},
	file = {ScienceDirect Snapshot:/home/jlehtonen/Zotero/storage/Y8ZUG298/S1751157718303298.html:text/html}
}

@book{bellis_bibliometrics_2009,
	title = {Bibliometrics and {Citation} {Analysis}: {From} the {Science} {Citation} {Index} to {Cybermetrics}},
	isbn = {978-0-8108-6714-7},
	shorttitle = {Bibliometrics and {Citation} {Analysis}},
	abstract = {Can the methods of science be directed toward science itself? How did it happen that scientists, scientific documents, and their bibliographic links came to be regarded as mathematical variables in abstract models of scientific communication? What is the role of quantitative analyses of scientific and technical documentation in current science policy and management? Bibliometrics and Citation Analysis: From the Science Citation Index to Cybermetrics answers these questions through a comprehensive overview of theories, techniques, concepts, and applications in the interdisciplinary and steadily growing field of bibliometrics.Since citation indexes came into the limelight during the mid-1960s, citation networks have become increasingly important for many different research fields. The book begins by investigating the empirical, philosophical, and mathematical foundations of bibliometrics, including its beginnings with the Science Citation Index, the theoretical framework behind it, and its mathematical underpinnings. It then examines the application of bibliometrics and citation analysis in the sciences and science studies, especially the sociology of science and science policy. Finally it provides a view of the future of bibliometrics, exploring in detail the ongoing extension of bibliometric methods to the structure and dynamics of the World Wide Web. This book gives newcomers to the field of bibliometrics an accessible entry point to an entire research tradition otherwise scattered through a vast amount of journal literature. At the same time, it brings to the forefront the cross-disciplinary linkages between the various fields (sociology, philosophy, mathematics, politics) that intersect at the crossroads of citation analysis. Because of its discursive and interdisciplinary approach, the book is useful to those in every area of scholarship involved in the quantitative analysis of information exchanges, but also to science historians and general readers who simply wish to familiarize them},
	language = {en},
	publisher = {Scarecrow Press},
	author = {Bellis, Nicola De},
	month = mar,
	year = {2009},
	note = {Google-Books-ID: ma4YjaKyM9cC},
	keywords = {Language Arts \& Disciplines / Library \& Information Science / General, Science / Research \& Methodology}
}

@article{cole_history_1917,
	title = {{THE} {HISTORY} {OF} {COMPARATIVE} {ANATOMY}: {PART} {I}.—{A} {STATISTICAL} {ANALYSIS} {OF} {THE} {LITERATURE}},
	volume = {11},
	issn = {2059-495X},
	shorttitle = {{THE} {HISTORY} {OF} {COMPARATIVE} {ANATOMY}},
	url = {https://www.jstor.org/stable/43426882},
	number = {44},
	urldate = {2019-06-18},
	journal = {Science Progress (1916-1919)},
	author = {COLE, F. J. and EALES, NELLIE B.},
	year = {1917},
	pages = {578--596}
}

@misc{noauthor_ministry_2020,
	title = {Ministry of {Education} and {Culture}, {Vipunen} - {Education} {Statistics} {Finland}, {Bibliometrics} ({Web} of {Science}) service},
	shorttitle = {Bibliometrics ({Web} of {Science}) service},
	url = {https://vipunen.fi/fi-fi/kkyhteiset/Sivut/Bibliometriikka.aspx},
	urldate = {2019-08-07},
	journal = {Bibliometriikka (Web of Science)},
	year = {2020},
	file = {Bibliometriikka (Web of Science):/home/jlehtonen/Zotero/storage/FRKEWIA6/Bibliometriikka.html:text/html}
}

@misc{noauthor_ministry_2020-1,
	title = {Ministry of {Education} and {Culture}, {Vipunen} - {Education} {Statistics} {Finland}, {Bibliometrics} ({Scopus}) service},
	shorttitle = {Bibliometrics ({Scopus}) service},
	url = {https://vipunen.fi/fi-fi/kkyhteiset/Sivut/Bibliometriikka-(Scopus).aspx},
	urldate = {2019-08-07},
	journal = {Bibliometriikka (Scopus)},
	year = {2020},
	file = {Bibliometriikka (Scopus):/home/jlehtonen/Zotero/storage/9NM7YV8Q/Bibliometriikka-(Scopus).html:text/html}
}

@article{blei_latent_2003,
	title = {Latent {Dirichlet} {Allocation}},
	volume = {3},
	issn = {ISSN 1533-7928},
	url = {http://jmlr.csail.mit.edu/papers/v3/blei03a.html},
	number = {Jan},
	urldate = {2019-08-16},
	journal = {Journal of Machine Learning Research},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	year = {2003},
	pages = {993--1022},
	file = {Full Text PDF:/home/jlehtonen/Zotero/storage/CC9S6KWD/Blei et al. - 2003 - Latent Dirichlet Allocation.pdf:application/pdf;Snapshot:/home/jlehtonen/Zotero/storage/CNWFAAY4/blei03a.html:text/html}
}

@article{calinski_dendrite_1974,
	title = {A dendrite method for cluster analysis},
	volume = {3},
	issn = {0361-0926},
	url = {http://www.tandfonline.com/doi/abs/10.1080/03610927408827101},
	doi = {10.1080/03610927408827101},
	language = {en},
	number = {1},
	urldate = {2019-11-13},
	journal = {Communications in Statistics - Theory and Methods},
	author = {Calinski, T. and Harabasz, J.},
	year = {1974},
	pages = {1--27},
	file = {Calinski and Harabasz - 1974 - A dendrite method for cluster analysis.pdf:/home/jlehtonen/Zotero/storage/HXF35TCL/Calinski and Harabasz - 1974 - A dendrite method for cluster analysis.pdf:application/pdf}
}

@article{pudovkin_algorithmic_2002,
	title = {Algorithmic procedure for finding semantically related journals},
	volume = {53},
	issn = {1532-2890},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/asi.10153},
	doi = {10.1002/asi.10153},
	abstract = {Using citations, papers and references as parameters a relatedness factor (RF) is computed for a series of journals. Sorting these journals by the RF produces a list of journals most closely related ...},
	language = {en},
	number = {13},
	urldate = {2020-05-03},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Pudovkin, Alexander I. and Garfield, Eugene},
	month = nov,
	year = {2002},
	note = {Publisher: John Wiley \& Sons, Ltd},
	pages = {1113--1119},
	file = {Snapshot:/home/jlehtonen/Zotero/storage/3U9A4UDW/asi.html:text/html}
}

@article{wang_large-scale_2016,
	title = {Large-scale analysis of the accuracy of the journal classification systems of {Web} of {Science} and {Scopus}},
	volume = {10},
	issn = {1751-1577},
	url = {http://www.sciencedirect.com/science/article/pii/S1751157715301930},
	doi = {10.1016/j.joi.2016.02.003},
	abstract = {Journal classification systems play an important role in bibliometric analyses. The two most important bibliographic databases, Web of Science and Scopus, each provide a journal classification system. However, no study has systematically investigated the accuracy of these classification systems. To examine and compare the accuracy of journal classification systems, we define two criteria on the basis of direct citation relations between journals and categories. We use Criterion I to select journals that have weak connections with their assigned categories, and we use Criterion II to identify journals that are not assigned to categories with which they have strong connections. If a journal satisfies either of the two criteria, we conclude that its assignment to categories may be questionable. Accordingly, we identify all journals with questionable classifications in Web of Science and Scopus. Furthermore, we perform a more in-depth analysis for the field of Library and Information Science to assess whether our proposed criteria are appropriate and whether they yield meaningful results. It turns out that according to our citation-based criteria Web of Science performs significantly better than Scopus in terms of the accuracy of its journal classification system.},
	language = {en},
	number = {2},
	urldate = {2020-05-06},
	journal = {Journal of Informetrics},
	author = {Wang, Qi and Waltman, Ludo},
	month = may,
	year = {2016},
	keywords = {Bibliographic database, Citation analysis, Journal classification system, Scopus, Web of Science},
	pages = {347--364},
	file = {ScienceDirect Full Text PDF:/home/jlehtonen/Zotero/storage/IKAUJJZX/Wang and Waltman - 2016 - Large-scale analysis of the accuracy of the journa.pdf:application/pdf;ScienceDirect Snapshot:/home/jlehtonen/Zotero/storage/TSXZVCSI/S1751157715301930.html:text/html}
}

@book{auranen_tieteen_2018,
	title = {Tieteen tila 2018 - {Suomen} {Akatemia}, [{Status} of science 2018 - {Academy} of {Finland}]},
	isbn = {978-951-715-902-9},
	url = {https://www.aka.fi/fi/tiedepoliittinen-toiminta/tieteen-tila/tieteen-tila-2018/},
	abstract = {Tieteen tila 2018 -katsauksessa tarkastellaan tutkimuksen resur-sointia yliopistoissa ja valtion tutkimuslaitoksissa tilastoaineisto-jen avulla. Julkaisutoimintaa, tieteellistä vaikuttavuutta sekä jul-kaisuyhteistyötä tarkastellaan bibliometrisin menetelmin. Biblio-metriset analyysit sisältävät kansainvälistä vertailua sekä tarkas-teluja tieteenalaryhmittäin ja tutkimusorganisaatioittain. Tieteen tila 2018 -katsausta varten tehtiin erityistarkastelut kansallisesta tavoitteesta nostaa tutkimus- ja kehittämistoiminnan panostuk-set neljään prosenttiin bruttokansantuotteesta sekä muutosteki-jöistä, jotka tulevaisuudessa vaikuttavat tutkimuksen sisältöihin ja tieteen tekemiseen. T\&k-panostusten neljän prosentin brutto-kansantuoteosuutta koskevaa tavoitetta tarkastellaan sekä tilas-toaineistoiden perusteella että keskustelevammin tieteen ja osaa-misen roolin näkökulmasta. Tieteen muutostekijöiden kartoitus on kokeiluluonteinen.},
	language = {Finnish},
	urldate = {2020-05-15},
	collaborator = {Auranen, Otto and Leskinen, Paula and Alho, Jussi and Nuutinen, Anu and Hemming, Samuli},
	year = {2018},
	file = {aka_tieteen_tila_2018_web.pdf:/home/jlehtonen/Zotero/storage/4FLCCCDR/aka_tieteen_tila_2018_web.pdf:application/pdf;Tieteen tila 2018 - Suomen Akatemia:/home/jlehtonen/Zotero/storage/Z674IBSE/tieteen-tila-2018.html:text/html}
}

@article{korenius_hierarchical_2006,
	title = {Hierarchical clustering of a {Finnish} newspaper article collection with graded relevance assessments},
	volume = {9},
	issn = {1386-4564, 1573-7659},
	url = {http://link.springer.com/10.1007/s10791-005-5720-6},
	doi = {10.1007/s10791-005-5720-6},
	abstract = {Search facilitated with agglomerative hierarchical clustering methods was studied in a collection of Finnish newspaper articles (N = 53,893). To allow quick experiments, clustering was applied to a sample (N = 5,000) that was reduced with principal components analysis. The dendrograms were heuristically cut to ﬁnd an optimal partition, whose clusters were compared with each of the 30 queries to retrieve the best-matching cluster. The fourlevel relevance assessment was collapsed into a binary one by (A) considering all the relevant and (B) only the highly relevant documents relevant, respectively. Single linkage (SL) was the worst method. It created many tiny clusters, and, consequently, searches enabled with it had high precision and low recall. The complete linkage (CL), average linkage (AL), and Ward’s methods (WM) returned reasonably-sized clusters typically of 18–32 documents. Their recall (A: 27–52\%, B: 50–82\%) and precision (A: 83–90\%, B: 18–21\%) was higher than and comparable to those of the SL clusters, respectively. The AL and WM clustering had 1–8\% better effectiveness than nearest neighbor searching (NN), and SL and CL were 1–9\% less efﬁcient that NN. However, the differences were statistically insigniﬁcant. When evaluated with the liberal assessment A, the results suggest that the AL and WM clustering offer better retrieval ability than NN. Assessment B renders the AL and WM clustering better than NN, when recall is considered more important than precision. The results imply that collections in the highly inﬂectional and agglutinative languages, such as Finnish, may be clustered as the collections in English, provided that documents are appropriately preprocessed.},
	language = {en},
	number = {1},
	urldate = {2020-05-15},
	journal = {Information Retrieval},
	author = {Korenius, Tuomo and Laurikkala, Jorma and Juhola, Martti and Järvelin, Kalervo},
	month = jan,
	year = {2006},
	keywords = {tulostettu},
	pages = {33--53},
	file = {Korenius et al. - 2006 - Hierarchical clustering of a Finnish newspaper art.pdf:/home/jlehtonen/Zotero/storage/9TFFDQ2S/Korenius et al. - 2006 - Hierarchical clustering of a Finnish newspaper art.pdf:application/pdf;Snapshot:/home/jlehtonen/Zotero/storage/2V9E38ZQ/10.html:text/html}
}

@techreport{persson_bibliometric_2000,
	address = {Espoo},
	title = {A {Bibliometric} {Study} of {Finnish} {Science}},
	copyright = {VTT, GROUP FOR TECHNOLOGY STUDIES},
	url = {http://www.vtt.fi/inf/julkaisut/muut/2000/wp48.pdf},
	language = {en},
	number = {48/00},
	institution = {VTT Technical Research Centre of Finland.},
	author = {Persson, Olle and Luukkonen, Terttu and Hälikkä, Sasu},
	year = {2000},
	pages = {81},
	file = {Persson et al. - A Bibliometric Study of Finnish Science.pdf:/home/jlehtonen/Zotero/storage/2J3NAIG4/Persson et al. - A Bibliometric Study of Finnish Science.pdf:application/pdf}
}

@article{ward_jr_hierarchical_1963,
	title = {Hierarchical {Grouping} to {Optimize} an {Objective} {Function}},
	volume = {58},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10500845},
	doi = {10.1080/01621459.1963.10500845},
	abstract = {A procedure for forming hierarchical groups of mutually exclusive subsets, each of which has members that are maximally similar with respect to specified characteristics, is suggested for use in large-scale (n {\textgreater} 100) studies when a precise optimal solution for a specified number of groups is not practical. Given n sets, this procedure permits their reduction to n − 1 mutually exclusive sets by considering the union of all possible n(n − 1)/2 pairs and selecting a union having a maximal value for the functional relation, or objective function, that reflects the criterion chosen by the investigator. By repeating this process until only one group remains, the complete hierarchical structure and a quantitative estimate of the loss associated with each stage in the grouping can be obtained. A general flowchart helpful in computer programming and a numerical example are included.},
	number = {301},
	urldate = {2020-05-20},
	journal = {Journal of the American Statistical Association},
	author = {Ward, Jr, Joe H.},
	month = mar,
	year = {1963},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1963.10500845},
	pages = {236--244},
	file = {Snapshot:/home/jlehtonen/Zotero/storage/PLK5J6B4/01621459.1963.html:text/html;ward.pdf:/home/jlehtonen/Zotero/storage/CIKBTXJP/ward.pdf:application/pdf}
}

@inproceedings{liu_understanding_2010,
	address = {Sydney, Australia},
	title = {Understanding of {Internal} {Clustering} {Validation} {Measures}},
	isbn = {978-1-4244-9131-5},
	url = {http://ieeexplore.ieee.org/document/5694060/},
	doi = {10.1109/ICDM.2010.35},
	abstract = {Clustering validation has long been recognized as one of the vital issues essential to the success of clustering applications. In general, clustering validation can be categorized into two classes, external clustering validation and internal clustering validation. In this paper, we focus on internal clustering validation and present a detailed study of 11 widely used internal clustering validation measures for crisp clustering. From ﬁve conventional aspects of clustering, we investigate their validation properties. Experiment results show that ������ ������������������ is the only internal validation measure which performs well in all ﬁve aspects, while other measures have certain limitations in different application scenarios.},
	language = {en},
	urldate = {2020-05-25},
	booktitle = {2010 {IEEE} {International} {Conference} on {Data} {Mining}},
	publisher = {IEEE},
	author = {Liu, Yanchi and Li, Zhongmou and Xiong, Hui and Gao, Xuedong and Wu, Junjie},
	month = dec,
	year = {2010},
	pages = {911--916},
	file = {Liu et al. - 2010 - Understanding of Internal Clustering Validation Me.pdf:/home/jlehtonen/Zotero/storage/IWKK59XK/Liu et al. - 2010 - Understanding of Internal Clustering Validation Me.pdf:application/pdf}
}

@article{luhn_key_1960,
	title = {Key word-in-context index for technical literature (kwic index)},
	volume = {11},
	issn = {1936-6108},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.5090110403},
	doi = {10.1002/asi.5090110403},
	abstract = {A distinction is made between bibliographical indexes for new and past literature based on the willingness of the user to trade perfection for currency. Indexes giving keywords in their context are proposed as suitable for disseminating new information. These can be entirely machine-generated and hence kept up-to-date with the current literature. A compatible coding scheme to identify the indexed documents is also proposed. In it elements are automatically extracted from the usual identifiers of the document so that the coded identifier yields a maximum of information while remaining susceptible to normal methods of ordering.},
	language = {en},
	number = {4},
	urldate = {2020-08-03},
	journal = {American Documentation},
	author = {Luhn, H. P.},
	year = {1960},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.5090110403},
	pages = {288--295},
	file = {Snapshot:/home/jlehtonen/Zotero/storage/HBDAYG6A/asi.html:text/html;Submitted Version:/home/jlehtonen/Zotero/storage/ZUI3QGKD/Luhn - 1960 - Key word-in-context index for technical literature.pdf:application/pdf}
}

@article{siemens_lemmatization_1996,
	title = {Lemmatization and parsing with \textit{{TACT}} preprocessing programs},
	copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
	issn = {1918-3666},
	url = {http://www.digitalstudies.org/articles/10.16995/dscn.233/},
	doi = {10.16995/dscn.233},
	abstract = {Article: Lemmatization and parsing with \textit{TACT} preprocessing programs},
	language = {en},
	number = {1},
	urldate = {2020-08-06},
	journal = {Digital Studies/Le champ numérique},
	author = {Siemens, R.},
	month = feb,
	year = {1996},
	note = {Number: 1
Publisher: Open Library of Humanities},
	file = {Snapshot:/home/jlehtonen/Zotero/storage/TBA3JRP2/dscn.233.html:text/html}
}

@article{hann_towards_1975,
	title = {Towards an {Algorithmic} {Methodology} of {Lemmatization}},
	volume = {3},
	url = {http://search.proquest.com/docview/1297799939/},
	abstract = {Introduced are algorithmic techniques enabling a computer to determine those inflected word forms in a textual corpus which are lexically equivalent, \& to automatically generate character-string transformations for reducing the raw source text to a series of lemmata. The proposed method is not linked to a specific language \& is applicable to the reduction of both grammatical \& syntactic inflections. In the 1st section, 3 types of inflexion are distinguished according to the respective occurrence domain within word-forms (initial, medial, \& terminal); the collection of discrete grammatical inflexions for German is defined in an easily programmed manner, \& criteria are suggested for the machine-detection of lexical equivalence. The 2nd section describes how the machine is able to process the raw text, rapidly obtain sets of equivalent word-forms, \& generate transformations for lemmatizing not only those word-forms with directly perceivable reduced forms, but also those whose lemma forms do not themselves occur in the corpus. In the 3rd section, the results of a practical investigation into the merits of algorithmic lemmatization for German are given. The non-technical presentation is geared to linguist as well as computer specialists. S. Karganovic},
	language = {English},
	number = {2},
	urldate = {2020-08-06},
	journal = {Association for Literary and Linguistic Computing Bulletin},
	author = {Hann, Michael L.},
	year = {1975},
	note = {Num Pages: 11},
	keywords = {algorithmic methodology of lemmatization, lexically equivalent inflected word-forms},
	pages = {140--150}
}

@article{trunk_problem_1979,
	title = {A {Problem} of {Dimensionality}: {A} {Simple} {Example}},
	volume = {PAMI-1},
	issn = {1939-3539},
	shorttitle = {A {Problem} of {Dimensionality}},
	doi = {10.1109/TPAMI.1979.4766926},
	abstract = {In pattern recognition problems it has been noted that beyond a certain point the inclusion of additional parameters (that have been estimated) leads to higher probabilities of error. A simple problem has been formulated where the probability of error approaches zero as the dimensionality increases and all the parameters are known; on the other hand, the probability of error approaches one-half as the dimensionality increases and parameters are estimated.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Trunk, G. V.},
	month = jul,
	year = {1979},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Correlators, Covariance matrix, Detectors, Dimensionality, Error analysis, Gaussian distribution, Parameter estimation, Pattern recognition, Probability, Reactive power, Testing},
	pages = {306--307},
	file = {IEEE Xplore Abstract Record:/home/jlehtonen/Zotero/storage/LWZPR5W4/4766926.html:text/html;IEEE Xplore Full Text PDF:/home/jlehtonen/Zotero/storage/QJPM5C97/Trunk - 1979 - A Problem of Dimensionality A Simple Example.pdf:application/pdf}
}

@misc{noauthor_national_2020,
	title = {National {Library} of {Medicine} {Classification} {Home} {Page}},
	url = {https://www.nlm.nih.gov/class/index.html},
	urldate = {2020-08-10},
	year = {2020},
	file = {NLM Classification Home Page:/home/jlehtonen/Zotero/storage/79LC7LAE/index.html:text/html}
}

@misc{noauthor_american_2020,
	title = {American {Physical} {Society} {Physics} {Subject} {Headings} home page.},
	url = {https://physh.aps.org/about},
	urldate = {2020-08-10},
	year = {2020},
	file = {PhySH — About PhySH:/home/jlehtonen/Zotero/storage/BB2FBD77/about.html:text/html}
}

@misc{noauthor_2012_2020,
	title = {The 2012 {ACM} {Computing} {Classification} {System}},
	url = {https://dl-acm-org.libproxy.aalto.fi/ccs},
	urldate = {2020-08-10},
	year = {2020},
	file = {Computing Classification System:/home/jlehtonen/Zotero/storage/YX2V5Y77/ccs.html:text/html}
}

@misc{manning_introduction_2008,
	title = {Introduction to {Information} {Retrieval}},
	url = {/core/books/introduction-to-information-retrieval/669D108D20F556C5C30957D63B5AB65C},
	abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.},
	language = {en},
	urldate = {2020-08-11},
	journal = {Cambridge Core},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	month = jul,
	year = {2008},
	doi = {10.1017/CBO9780511809071},
	note = {ISBN: 9780521865715 9780511809071
Publisher: Cambridge University Press},
	file = {Snapshot:/home/jlehtonen/Zotero/storage/96GINUMX/669D108D20F556C5C30957D63B5AB65C.html:text/html}
}

@article{luhn_statistical_1957,
	title = {A {Statistical} {Approach} to {Mechanized} {Encoding} and {Searching} of {Literary} {Information}},
	volume = {1},
	issn = {0018-8646},
	doi = {10.1147/rd.14.0309},
	abstract = {Written communication of ideas is carried out on the basis of statistical probability in that a writer chooses that level of subject specificity and that combination of words which he feels will convey the most meaning. Since this process varies among individuals and since similar ideas are therefore relayed at different levels of specificity and by means of different words, the problem of literature searching by machines still presents major difficulties. A statistical approach to this problem will be outlined and the various steps of a system based on this approach will be described. Steps include the statistical analysis of a collection of documents in a field of interest, the establishment of a set of “notions” and the vocabulary by which they are expressed, the compilation of a thesaurus-type dictionary and index, the automatic encoding of documents by machine with the aid of such a dictionary, the encoding of topological notations (such as branched structures), the recording of the coded information, the establishment of a searching pattern for finding pertinent information, and the programming of appropriate machines to carry out a search.},
	number = {4},
	journal = {IBM Journal of Research and Development},
	author = {Luhn, H. P.},
	month = oct,
	year = {1957},
	note = {Conference Name: IBM Journal of Research and Development},
	pages = {309--317},
	file = {IEEE Xplore Abstract Record:/home/jlehtonen/Zotero/storage/HVABE2PS/5392697.html:text/html;Submitted Version:/home/jlehtonen/Zotero/storage/QRWZSFQI/Luhn - 1957 - A Statistical Approach to Mechanized Encoding and .pdf:application/pdf}
}

@article{jones_statistical_1972,
	title = {A statistical interpretation of term specificity and its application in retrieval},
	volume = {28},
	abstract = {Abstract: The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing, in particular, that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure. Exhaustivity and specificity We are familiar with the notions of exhaustivity and specificity: exhaustivity is a property of index descriptions, and specificity one of index terms. They are most clearly illustrated by a simple keyword or descriptor system. In this case the exhaustivity of a document description is the coverage of its various topics given by the terms assigned to it; and the specificity of an individual term is the level of detail at which a given concept is represented.},
	journal = {Journal of Documentation},
	author = {Jones, Karen Spärck},
	year = {1972},
	pages = {11--21},
	file = {Citeseer - Full Text PDF:/home/jlehtonen/Zotero/storage/KH8P6CY3/Jones - 1972 - A statistical interpretation of term specificity a.pdf:application/pdf;Citeseer - Snapshot:/home/jlehtonen/Zotero/storage/5UDZ6HF3/summary.html:text/html}
}

@article{dumais_latent_2004,
	title = {Latent semantic analysis},
	volume = {38},
	issn = {0066-4200},
	url = {http://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/aris.1440380105},
	doi = {10.1002/aris.1440380105},
	number = {1},
	urldate = {2020-08-11},
	journal = {Annual Review of Information Science and Technology},
	author = {Dumais, Susan T.},
	month = jan,
	year = {2004},
	note = {Publisher: John Wiley \& Sons, Ltd},
	pages = {188--230},
	file = {Snapshot:/home/jlehtonen/Zotero/storage/44QFL2YW/aris.html:text/html}
}

@article{murtagh_wards_2011,
	title = {Ward's {Hierarchical} {Clustering} {Method}: {Clustering} {Criterion} and {Agglomerative} {Algorithm}},
	volume = {1111},
	shorttitle = {Ward's {Hierarchical} {Clustering} {Method}},
	url = {http://adsabs.harvard.edu/abs/2011arXiv1111.6285M},
	abstract = {The Ward error sum of squares hierarchical clustering method has been 
very widely used since its first description by Ward in a 1963
publication. It has also been generalized in various ways. However there
are different interpretations in the literature and there are different
implementations of the Ward agglomerative algorithm in commonly used
software systems, including differing expressions of the agglomerative
criterion. Our survey work and case studies will be useful for all those
involved in developing software for data analysis using Ward's
hierarchical clustering method.},
	urldate = {2020-08-11},
	journal = {arXiv e-prints},
	author = {Murtagh, Fionn and Legendre, Pierre},
	month = nov,
	year = {2011},
	keywords = {62H30, Computer Science - Computer Vision and Pattern Recognition, G.3, H.3.3, Statistics - Machine Learning, 91C20, Statistics - Applications},
	pages = {arXiv:1111.6285},
	file = {Full Text PDF:/home/jlehtonen/Zotero/storage/HWF4UALQ/Murtagh and Legendre - 2011 - Ward's Hierarchical Clustering Method Clustering .pdf:application/pdf}
}

@article{strauss_generalising_2017,
	title = {Generalising {Ward}’s {Method} for {Use} with {Manhattan} {Distances}},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0168288},
	doi = {10.1371/journal.pone.0168288},
	abstract = {The claim that Ward’s linkage algorithm in hierarchical clustering is limited to use with Euclidean distances is investigated. In this paper, Ward’s clustering algorithm is generalised to use with l1 norm or Manhattan distances. We argue that the generalisation of Ward’s linkage method to incorporate Manhattan distances is theoretically sound and provide an example of where this method outperforms the method using Euclidean distances. As an application, we perform statistical analyses on languages using methods normally applied to biology and genetic classification. We aim to quantify differences in character traits between languages and use a statistical language signature based on relative bi-gram (sequence of two letters) frequencies to calculate a distance matrix between 32 Indo-European languages. We then use Ward’s method of hierarchical clustering to classify the languages, using the Euclidean distance and the Manhattan distance. Results obtained from using the different distance metrics are compared to show that the Ward’s algorithm characteristic of minimising intra-cluster variation and maximising inter-cluster variation is not violated when using the Manhattan metric.},
	language = {en},
	number = {1},
	urldate = {2020-08-11},
	journal = {PLOS ONE},
	author = {Strauss, Trudie and Maltitz, Michael Johan von},
	month = jan,
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Clustering algorithms, Distance measurement, Hierarchical clustering, Languages, Organismal evolution, Phylogenetic analysis, Phylogenetics},
	pages = {e0168288},
	file = {Full Text PDF:/home/jlehtonen/Zotero/storage/32ITW577/Strauss and Maltitz - 2017 - Generalising Ward’s Method for Use with Manhattan .pdf:application/pdf;Snapshot:/home/jlehtonen/Zotero/storage/MMJHAXPV/article.html:text/html}
}

@misc{noauthor_princeton_2010,
	title = {Princeton {University} "{About} {WordNet}." {WordNet}. {Princeton} {University}},
	url = {https://wordnet.princeton.edu/},
	urldate = {2020-08-11},
	journal = {Citing WordNet {\textbar} WordNet},
	year = {2010},
	file = {Citing WordNet | WordNet:/home/jlehtonen/Zotero/storage/QICB2D88/citing-wordnet.html:text/html}
}

@inproceedings{dumais_using_1988,
	address = {New York, NY, USA},
	series = {{CHI} '88},
	title = {Using latent semantic analysis to improve access to textual information},
	isbn = {978-0-201-14237-2},
	url = {https://doi.org/10.1145/57167.57214},
	doi = {10.1145/57167.57214},
	abstract = {This paper describes a new approach for dealing with the vocabulary problem in human-computer interaction. Most approaches to retrieving textual materials depend on a lexical match between words in users' requests and those in or assigned to database objects. Because of the tremendous diversity in the words people use to describe the same object, lexical matching methods are necessarily incomplete and imprecise [5]. The latent semantic indexing approach tries to overcome these problems by automatically organizing text objects into a semantic structure more appropriate for matching user requests. This is done by taking advantage of implicit higher-order structure in the association of terms with text objects. The particular technique used is singular-value decomposition, in which a large term by text-object matrix is decomposed into a set of about 50 to 150 orthogonal factors from which the original matrix can be approximated by linear combination. Terms and objects are represented by 50 to 150 dimensional vectors and matched against user queries in this “semantic” space. Initial tests find this completely automatic method widely applicable and a promising way to improve users' access to many kinds of textual materials, or to objects and services for which textual descriptions are available.},
	urldate = {2020-08-12},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dumais, S. T. and Furnas, G. W. and Landauer, T. K. and Deerwester, S. and Harshman, R.},
	month = may,
	year = {1988},
	pages = {281--285},
	file = {Full Text PDF:/home/jlehtonen/Zotero/storage/XYQH2KRU/Dumais et al. - 1988 - Using latent semantic analysis to improve access t.pdf:application/pdf}
}

@article{kimes_statistical_2017,
	title = {Statistical significance for hierarchical clustering},
	volume = {73},
	copyright = {© 2017, The International Biometric Society},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12647},
	doi = {10.1111/biom.12647},
	abstract = {Cluster analysis has proved to be an invaluable tool for the exploratory and unsupervised analysis of high-dimensional datasets. Among methods for clustering, hierarchical approaches have enjoyed substantial popularity in genomics and other fields for their ability to simultaneously uncover multiple layers of clustering structure. A critical and challenging question in cluster analysis is whether the identified clusters represent important underlying structure or are artifacts of natural sampling variation. Few approaches have been proposed for addressing this problem in the context of hierarchical clustering, for which the problem is further complicated by the natural tree structure of the partition, and the multiplicity of tests required to parse the layers of nested clusters. In this article, we propose a Monte Carlo based approach for testing statistical significance in hierarchical clustering which addresses these issues. The approach is implemented as a sequential testing procedure guaranteeing control of the family-wise error rate. Theoretical justification is provided for our approach, and its power to detect true clustering structure is illustrated through several simulation studies and applications to two cancer gene expression datasets.},
	language = {en},
	number = {3},
	urldate = {2020-08-12},
	journal = {Biometrics},
	author = {Kimes, Patrick K. and Liu, Yufeng and Hayes, David Neil and Marron, James Stephen},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.12647},
	keywords = {High-dimension, Hypothesis testing, Multiple correction, Unsupervised learning},
	pages = {811--821},
	file = {Full Text PDF:/home/jlehtonen/Zotero/storage/Z4HTEMQU/Kimes et al. - 2017 - Statistical significance for hierarchical clusteri.pdf:application/pdf;Snapshot:/home/jlehtonen/Zotero/storage/7QPCMSQF/biom.html:text/html}
}

@inproceedings{aggarwal_surprising_2001,
	address = {Berlin, Heidelberg},
	series = {{ICDT} '01},
	title = {On the {Surprising} {Behavior} of {Distance} {Metrics} in {High} {Dimensional} {Spaces}},
	isbn = {978-3-540-41456-8},
	abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a efficiency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used Lk norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric (L1 norm) is consistently more preferable than the Euclidean distance metric (L2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the Lk norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
	urldate = {2020-08-12},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Database} {Theory}},
	publisher = {Springer-Verlag},
	author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
	month = jan,
	year = {2001},
	pages = {420--434}
}

@article{hodge_survey_2004,
	title = {A survey of outlier detection methodologies},
	volume = {22},
	abstract = {Abstract. Outlier detection has been used for centuries to detect and, where appropriate, remove anomalous observations from data. Outliers arise due to mechanical faults, changes in system behaviour, fraudulent behaviour, human error, instrument error or simply through natural deviations in populations. Their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences. It can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing. The original outlier detection methods were arbitrary but now, principled and systematic techniques are used, drawn from the full gamut of Computer Science and Statistics. In this paper, we introduce a survey of contemporary techniques for outlier detection. We identify their respective motivations and distinguish their advantages and disadvantages in a comparative review.},
	journal = {Artificial Intelligence Review},
	author = {Hodge, Victoria J. and Austin, Jim},
	year = {2004},
	pages = {2004},
	file = {Citeseer - Full Text PDF:/home/jlehtonen/Zotero/storage/4PD2FXT7/Hodge and Austin - 2004 - A survey of outlier detection methodologies.pdf:application/pdf;Citeseer - Snapshot:/home/jlehtonen/Zotero/storage/RMLGABUC/summary.html:text/html}
}

@article{hubert_comparing_1985,
	title = {Comparing partitions},
	volume = {2},
	issn = {1432-1343},
	url = {https://doi.org/10.1007/BF01908075},
	doi = {10.1007/BF01908075},
	abstract = {The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Miligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between ±1.},
	language = {en},
	number = {1},
	urldate = {2020-08-13},
	journal = {Journal of Classification},
	author = {Hubert, Lawrence and Arabie, Phipps},
	month = dec,
	year = {1985},
	pages = {193--218}
}

@article{dolnicar_review_2002,
	title = {A {Review} of {Unquestioned} {Standards} in {Using} {Cluster} {Analysis} for {Data}-{Driven} {Market} {Segmentation}},
	url = {https://ro.uow.edu.au/commpapers/273},
	journal = {Faculty of Commerce - Papers (Archive)},
	author = {Dolnicar, Sara},
	month = dec,
	year = {2002},
	file = {"A Review of Unquestioned Standards in Using Cluster Analysis for Data-" by Sara Dolnicar:/home/jlehtonen/Zotero/storage/N5VJ549H/273.html:text/html}
}

@article{rousseeuw_silhouettes:_1987,
	title = {Silhouettes: {A} graphical aid to the interpretation and validation of cluster analysis},
	volume = {20},
	issn = {03770427},
	shorttitle = {Silhouettes},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0377042787901257},
	doi = {10.1016/0377-0427(87)90125-7},
	abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects he well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate’ number of clusters.},
	language = {en},
	urldate = {2020-08-15},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Rousseeuw, Peter J.},
	month = nov,
	year = {1987},
	keywords = {cluster analysis, classification, clustering validity, Graphical display},
	pages = {53--65},
	file = {Rousseeuw - 1987 - Silhouettes A graphical aid to the interpretation.pdf:/home/jlehtonen/Zotero/storage/VEEPUHQC/Rousseeuw - 1987 - Silhouettes A graphical aid to the interpretation.pdf:application/pdf;ScienceDirect Snapshot:/home/jlehtonen/Zotero/storage/JZGJNGDM/0377042787901257.html:text/html}
}

@inproceedings{halkidi_clustering_2001,
	title = {Clustering validity assessment: finding the optimal partitioning of a data set},
	shorttitle = {Clustering validity assessment},
	doi = {10.1109/ICDM.2001.989517},
	abstract = {Clustering is a mostly unsupervised procedure and the majority of clustering algorithms depend on certain assumptions in order to define the subgroups present in a data set. As a consequence, in most applications the resulting clustering scheme requires some sort of evaluation regarding its validity. In this paper we present a clustering validity procedure, which evaluates the results of clustering algorithms on data sets. We define a validity index, S Dbw, based on well-defined clustering criteria enabling the selection of optimal input parameter values for a clustering algorithm that result in the best partitioning of a data set. We evaluate the reliability of our index both theoretically and experimentally, considering three representative clustering algorithms run on synthetic and real data sets. We also carried out an evaluation study to compare S Dbw performance with other known validity indices. Our approach performed favorably in all cases, even those in which other indices failed to indicate the correct partitions in a data set.},
	booktitle = {Proceedings 2001 {IEEE} {International} {Conference} on {Data} {Mining}},
	author = {Halkidi, M. and Vazirgiannis, M.},
	month = nov,
	year = {2001},
	keywords = {clustering algorithms, Clustering algorithms, clustering validity assessment, data mining, Data visualization, Geometry, Humans, Informatics, Multidimensional systems, optimal partitioning data set, Partitioning algorithms, pattern clustering, Radio access networks, reliability, Reliability theory, SDbw validity index, Visual perception},
	pages = {187--194},
	file = {IEEE Xplore Abstract Record:/home/jlehtonen/Zotero/storage/K9T75I8C/989517.html:text/html;IEEE Xplore Full Text PDF:/home/jlehtonen/Zotero/storage/EGLTPAPI/Halkidi and Vazirgiannis - 2001 - Clustering validity assessment finding the optima.pdf:application/pdf}
}
