\chapter{Data and Methods}
\label{chapter:methods}
In this chapter we present the data and methods used in the 
clustering. We follow the logical order the methods are applied on 
the data.

\section{Publication metadata}
\label{section:metadata}
The data consits of X records of Clarivate's Web of Science 
publication metadata from year Y.

Our data here consists of short metadata records describing the 
publications. An example of a shortened record:
\begin{verbatim}
 Lehti: ACTA OPHTHALMOLOGICA SCANDINAVICA
 ISSN: 1395-3907
 Ala: OPHTHALMOLOGY
 Ilmestymisvuosi:   1999
 Otsikko: Assessment of diabetic retinopathy using two-field 60 
 degrees fundus photography. A comparison between[...]
 Abstrakti
 Purpose: To assess the severity of diabetic retinopathy and 
 maculopathy bycomparing[...]
 Avainsana (KeywordPlus):  OPHTHALMOSCOPY
 Avainsana (KeywordPlus):  KAPPA
 [...]
 Avainsana (tekijät):  diabetic retinopathy
 Avainsana (tekijät):  diabetic maculopathy
 [...]
 Lähde: 0010603696 /  *DIAB CONTR COMPL /  NEW ENGL J MED /  977 
 /  329 /  1993
 Lähde: 0034118371 /  *DIAB CONTR COMPL /  ARCH OPHTHALMOL-CHI /  
 1344 /  105 /  1987
 Lähde: 0075276068 /  *DRS RES GROUP /  OPHTHALMOLOGY /  82 /  85 
 /  1978
 \end{verbatim}
 
Each metadata record contains the title (``\texttt{Lehti}''), 
ISSN, the field of science (``\texttt{Ala}'') and publication 
year (``\texttt{Ilmestymisvuosi}'') of the journal, the title 
(``\texttt{Otsikko}'') and the abstract of the article 
(``\texttt{Abstrakti}''), keywords produced by algorithm 
of the publisher (``\texttt{Avainsana (KeywordPlus)}'') and the 
authors themselves (``\texttt{Avainsana (tekijät)}'') and the 
cited references (``\texttt{Lähde}'').
% \fixme{Tarkista YL:ltä että lähteet ovat nimenomaan 
% artikkelin omat. (Viitteet voivat olla metodologisia, 
% historiallisia ja siten johtaa harhaan). Toisaalta en nyt käytä 
% työssäni viitteitä (keskustelu LM:n kanssa 20.9.).} OK


 
\section{Analyzing textual data}
Basically the descriptions are natural English 
language appended with the citation references. When analyzing 
this kind of textual data the often used methods involve some 
kind of counting. We count, for example, which
are the most used words, which words appear together and so on.
% We can count n-grams.
This is called text analysis. One of the main principles was made 
popular by George Kingsley Zipf. He stated that the frequency of 
a word in any corpus is approximately inversely proportional to 
its rank. \fixme{This needs clarification/pruning}

Next we will describe the methods used in this work.


\subsection{Preprocessing}
The preprocessing for text analysis usually also includes the 
removing of \emph{the stop words} from data. Stop words are the 
most frequent words in the data like: \emph{``the, of, and, where 
etc.''} Because these words are present in any text they probably 
don't tell much about the topics these records concern. 
\cite{ref_here}


\subsection{Lemmatizing}
After removing stop words the next step is to unify the 
different written forms a concept. We might have eg. ``visual'' 
and ``visually'' or ``dog'' and ``dogs'' and we want them as 
``visual'' and ``dog'' only. This is desirable to reduce the 
redundant repetion the data and also to reduce the dimensions of 
the feature space. [...] \cite{ref_here}
There are two possible options to achieve this. 

\emph{Lemmatizing} means replacing each inflectional form of a 
word with its nominative form, or lemma. The context of the word 
is used to decide the correct lemma in the case of a word with 
multiple meanings. \fixme{In other words, words surrounding the 
word to be lemmatized... explain}  Nevertheless these homographic 
word forms can be problematic and cause ambiguity. The same 
lemmatizing should also be done to the stop words before they are 
removed.

\emph{Stemming} means stripping the word of its termination such 
that only the stem of the word is retained. No context is used 
and only the word itself is inspected.

Lemmatization results in better precision, or true negative rate 
but poorer recall true positive rate compared to stemming. 
\cite{ref_here}


\subsection{Vectorizing}
The clustering algorithms don't understand text documents but 
require numerical input. To enable the handling of the textual 
data by the clustering algorithms we have to transform it into 
numerical form. This is called vectorizing.

After we have lemmatized the terms we count the occurences 
of each term in each document. The resulting term occurence 
frequencies are normalized to decrease the importance of the 
tokens that occur in the majority of documents. Usually these are 
common terms not specific to the topic of the document. 
These normalized occurence frequencies are called term 
frequency inverse document frequencies, \textbf{TF-IDF}, and 
they form the features of a document. The size of the feature 
space is determined by the number of counted unique terms in all 
documents of the collection. \fixme{Kaava tähän, lähde, TF-IDF}

Here the size of the feature space is in the order of the the 
size of the English language corpus ie. in tens of thousands. The 
document feature matrix is very sparse, a single document only 
having handful of terms of the whole corpus. \fixme{Tähän 
esimerkiksi ko. matriisin palanen?}
% 29.4.2020 Vain prosenttiosuus tiheydestä



\section{Dimensionality reduction}
\label{sec:dimensionalityreduction}
The textual data is encoded with bag of words model, where 
meaning that for each word in the corpus, in this case almost 
whole English language, the record has a binary feature that is 1 
if that word occurs in the record and 0 if not, the resulting 
record-feature matrix has the dimensionality of $m*n$ where $m$ in 
the number of records and $n$ is the number of features and very 
sparse. In our case the record-feature matrix has the 
dimensionality approximately $10000*20000$. 


\subsection{Singular value decomposition}
Singular value decomposition (SVD) is a common method to reduce 
the dimensionality of data. 
\fixme{Vertaa/havainnollista miten eroaa PCA:sta}


\section{Model selection}
Generally all machine learning problems are ill-posed in the sense 
that a unique solution for the problem can't be found unless some
assumptions, or \emph{inductive bias}, are introduced. This begins 
with selecting the learning algorithm and might also include some
hyperparameters of selected algorithm.

Here we will use unsupervised learning to shape the mapping of 
scientific disciplines because we want to learn the possible 
intristic structure of sciencetific knowledge.  
There are many different clustering algorithms from which to 
choose. Some often used common algorithms are k-means, hierarchical
clustering, density based scan clustering and Gaussian clustering.
We will use hierarchical clustering because it's quite simple and 
familiar to us.
In hierarchical clustering there are yet different parameters to 
choose. 
% We will handle those in later chapters.
Hierarchical clustering with single, average and complete linkages
and Ward's method applied to search query result clustering were 
studied by Korenius et al. \cite{korenius_hierarchical_2006}.

\subsection{Choosing the number of clusters}
The number of clusters $k$ that should result from the clustering is
not known beforehand but is required a parameter of the
clustering algorithm. Hierarchical clustering will always return 
$k$ clusters regardless if they are meaningful or not. To make a
decision about the number of clusters we can 1) inspect the data in 
two dimensions using some dimensionality reduction method such as 
PCA and then plot it, 2) set limits for the inter-cluster 
distances at each step of merging clusters ie. if finding the 
largest gap between the dendrogram levels or 3) measuring the 
overall compactness of clusters and their separation from each 
other over the number clusters $k$ with suitable evaluation 
criterion such as average Silhouette value 
\cite{alpaydin2004introduction} \cite{calinski_dendrite_1974}(?) 
\cite{rousseeuw_silhouettes:_1987}. The first method is assumed to 
give uninformative view of the data. We will use the third method.
% Toinen vaihtoehto mahdollisesti samankaltainen kolmannen kanssa?





% TODO: Kirjoita nämä jonnekin näille main tai poista
% We need a method to measure the ``goodness'' of the clustering.
% As we work with text data, the dimensionality increases
% quite high and projecting data down to 2 or 3 dimensions for 
% visualization is not a simple task. (We come back to visualization 
% later though.)
% % LM 06/20: perustelut hyvin asian ongelmallisuudelle
% So we have to resort to measurements derived from the 
% resulting clustering itself. If we knew some underlying ground 
% truth behind our clustering problem, we could validate our result 
% against it. But as mentioned earlier, even defining what actually 
% are the current fields of science depends on who you ask and for 
% what purpose the definition is needed. So the ground truth is only
% one measure for our results here.
% In the lack of ground truth we can use some ``internal'' goodness 
% measure for the resulting clustering. These kind of measures 
% basically try to infer how dense the clusters are compared with how 
% sparse the inter-cluster space is and how well the clusters are 
% separated from each other.
% 
%\section{Silhouettes}
% One such measure to estimate the ``goodness'' of a clustering is 
% silhouettes. Silhouettes use average proximities that are know
% to work best with clear, compact and spherical clusters
% \cite{rousseeuw_silhouettes:_1987}. Silhouette value for an item
% is defined as:






\subsubsection{Internal validation of clustering results}
Liu et al. have reviewed 11 commmonly used internal clustering 
validation indices \cite{liu_understanding_2010}. These are used 
to decide the correct number of clusters. We will use two 
of those; Calinski-Harabasz criterion \cite{calinski_dendrite_1974} 
and Silhouette value \cite{rousseeuw_silhouettes:_1987}. We choose 
these because they were familiar for us and have different
weak spots as noted by Liu et al and thus could complement each 
other. Calinski-Harabasz criterion is defined as
\begin{equation}
 CH = \frac{SS_B}{N-k} \frac{SS_W}{k-1},
\end{equation}
where $SS_B$ is \emph{the between-group sum of squares}, $SS_W$ is the
within-group sum of squares, $k$ is the number of clusters 
and $N$ is the number of observations.

\emph{The between-group sum of squares} $SS_B$ gives the overall variance
between clusters
\begin{equation}
 SS_B = \sum_{i=1}^k n_i ||m_i-m||^2,
\end{equation}
where $k$ is the number of clusters, $n_i$ is the number of 
observations in cluster $i$, $m_i$ is the centroid of cluster $i$, 
$m$ is the overall mean of sample data and $||m_i-m||$ is the 
Euclidean distance between the two vectors.

The within-group sum of squares $SS_W$ gives the over variance 
within clusters
\begin{equation}
 SS_W = \sum_{i=1}^k \sum_{x\in c_i} ||x-m_i||^2,
\end{equation}
where $k$ is the number of clusters, $x$ is the observation, 
$c_i$ is the $i$th cluster, $m_i$ is the centroid of cluster $i$, and 
$||x-m_i||$ is the Euclidean distance between the two vectors.
The larger the Calinski-Harabasz criterion, the better the 
cluster structure.


\subsubsection{External validation of clustering results}
% Otsikko oli: Manually annotated validation set
% Gold standard set. Actually a gold standard set would be a set
% of all data sets with abstract excluding sets that don't have it.
For not to have to rely solely on internal clustering validation we
also try external clustering validation with small subset of data.
We will create a manually annotated validation set for calculating
precision, recall and other metrics derived from those.
% Käsin luokitellun aineiston kuvaus
The validation set consists of Finnish publications from year 2000
from three different fields of science as categorized by WOS.
Two of the fields are closely related sub fields of computer
science: \emph{computer science: information systems} (CS-IS) and 
\emph{computer science: 
artificial intelligence} (CS-AI), while the third one, 
\emph{clinical neurology}, is more distant from those. There are 
$116$, $122$, $250$ publications labeled as belonging to fields 
CS-IS, CS-AI and clinical neurology respectively, and $31$ 
publications labeled as belonging to both CS-IS and CS-AI, 
totalling $519$ publications in our data.
% Huonojen näytteiden poisto
Publications were inspected by title, abstract, keywords, journal
and publisher assigned disciplines of the journal. Publications
with critically missing data, unclear discipline assignment and
heavily applied publications (eg. a publication describing the 
development of a multiuser virtual community and a community game 
platform) were excluded from validation set. 
% Luokittelun varmistus käsin
We then checked by layman's reasoning if the WOS labeled 
discipline seemed plausible. Because of the journal based 
classification, most publications had more than one 
assigned discipline. Only discipline labels named 
previously were retained because we wanted to test 
how well our clustering method separated these three 
groups regardless of their labels. So essentially the 
labels could have been eg. 1, 2, 3.
% ``Väärin luokiteltujen uudelleen luokittelu''
In total 29 publications were manually reclassified by either
choosing only one of two used classes or by assigning the 
publication to more fitting category by our judgement.
% Tavoite
Goal was to achieve evaluation measurements based on a quite 
clearly separated set of publications. More vaguely classifiable
publications were included for comparison. We asked an another 
opinion for publications that could have been in more 
than one of our categories. For manually curated validation set 
with discipline assignment and justifications for
possible exclusion see appendix (Insert reference!).

The basic problem is that fields of science can not be 
defined so that they clearly differ from each other. Where one 
discipline end the other has already started like metallurgy and 
material science. Likewise, there are lots of publications that 
handle topics belonging to more than one discipline, eg. this 
thesis discusses clustering and bibliometrics. So when annotating 
publications, deciding if a publication belonged to
a discipline or not felt often quite difficult. Often the 
separation between disciplines felt quite arbitrary. For example
an article describing using wavelet transformation for coding noisy
images was decided to belong to CS information systems whereas an
article describing wavelet based corner detection using SVD was
decided to belong to CS artificial intelligence.
For CS artificial intelligence we mostly selected publications 
which mentioned some dimensionalty reduction or machine learning 
related term or concept. CS information systems ended up being 
quite like some ``others'' or
``the rest'' dump class. It would have publications such as
``A reference model for conceptualising the convergence of 
telecommunications and datacommunications service platforms'',
``Developing a distributed meeting service to support mobile 
meeting participants'',
``On voice quality of IP voice over GPRS'',
\fixme{Perustele vain yhteen alaan luokittelu. ``Tukeudun 
WOS-luokitteluun''.}
\fixme{Selvennä yläluokka-alaluokka-jako: CS general vs. CS 
information system tai CS artificial intelligence.}

\subsubsection{Precision and recall}
% määrittele peruskäsitteet kuten precision ja recall
Recall is the number of items correctly classified divided by the 
total number of items in that class.
\begin{equation}
 Recall = \frac{|\{relevant\ items\} \cap \{retrieved\ items\}|} 
{\{relevant\ items\}}
\end{equation}

Precision is the number of items correctly classified divided by 
the total number items classified into that class (ie. true and 
false positives).
\begin{equation}
 Precision = \frac{|\{relevant\ items\} \cap \{retrieved\ 
items\}|} 
{\{retrieved\ items\}}
\end{equation}


\section{Agglomerative clustering}
\label{sec:agglomerativeclustering}
In this work we use agglomerative hierarchical clustering with 
Ward's distance metric.\cite{ref_here} \fixme{Miksi, 
perustelut?} Agglomerative clustering starts with each document 
as a cluster of its own and then merges pairs of clusters together 
as the clustering process goes on.


\subsection{Distance metric}
Our data is high-dimensional so choosing distance measure is 
important. The higher the number of dimensions the more 
dissimilar each observation seems to from every other observation. 
This is known as \emph{curse of dimensionality} \fixme{citation}.
Boyack et al. have compared clustering real world size corpus of 
2.5 million publications from MEDLINE with nine different 
similarity metrics. \cite{boyack_clustering_2011}
Next we will briefly present some distance measures.

Cosine angle is invariant (uncentered Pearson 
correlation)\fixme{look: 
\url{https://www.researchgate.net/post/What_is_the_best_distance_
measure_for_high_dimensional_data/4}}

Euclidean distance is the familiar distance used in everyday life:
\begin{equation}
 ||a-b||_2 = \sqrt{\sum_i{(a_i-b_i)^2}}
\end{equation}

Mahalnobis distance...

We use Manhattan distance because it works well with sparse data.
\cite{ref_here}
\begin{equation}
 ||a-b||_1 = \sum_i{(a_i-b_i)}
\end{equation}


\subsection{Linkage methods}

% Vain mitä aion käyttää!
% Menetelmien kuvaus - mistä siinä on kyse!?!

Linkage methods affects how clusters are formed and how the 
distance metric is applied between two clusters.

\subsubsection{Single linkage}
Single linkage defines that the distance between two clusters is 
measured as distance between two closest items of the clusters 
$A$ and $B$.
\begin{equation}
 min\{d(a,b):a \in A, b \in B\}
\end{equation}

\subsubsection{Complete linkage}
Complete linkage is defined as the distance between the most 
distant items of clusters $A$ and $B$.
\begin{equation}
 max\{d(a,b):a \in A, b \in B\}
\end{equation}

\subsubsection{Average linkage}
Average linkage is defined as the average of the all pairwise 
distances between the items of cluster $A$ and $B$.
\begin{equation}
 \frac{1}{|A| \cdot |B|} \sum_{a \in A} \sum_{b \in B}d(a,b)
\end{equation}

\subsubsection{Ward's method}
% first 'ref_here': 
% www.statisticshowto.datasciencecentral.com/wards-method
We are using Ward's method because it usually creates compact 
even-sized clusters. \cite{ref_here} Ward's method minimizes the 
total within-cluster variance.
\begin{equation}
 definition\ here
\end{equation}



\subsection{Algorithmic complexity}
Both, time and space requirements of the used agglomerative clustering 
method are $O(N^2)$, where $N$ is the number of publications 
\cite{willett_recent_1988}. These are quite demanding requirements
when the number of publications increase. In this case it is 
acceptable because there is no need for real time updates of 
results, rather we experiment with exploratory data analysis.



