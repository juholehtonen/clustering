\chapter{Data and Methods}
\label{chapter:methods}
In this chapter we present the data and methods used in the 
clustering. We follow the logical order the methods are applied on 
the data.

\section{Publication metadata}
The data consits of X records of Clarivative Analytics' 
(formerly Thomson Reuters) Web of Science publication metadata 
from year Y.

Our data here consists of short metadata records describing the 
publications. An example of a shortened record:
\begin{verbatim}
 Lehti: ACTA OPHTHALMOLOGICA SCANDINAVICA
 ISSN: 1395-3907
 Ala: OPHTHALMOLOGY
 Ilmestymisvuosi:   1999
 Otsikko: Assessment of diabetic retinopathy using two-field 60 
 degrees fundus photography. A comparison between[...]
 Avainsana (KeywordPlus):  OPHTHALMOSCOPY
 Avainsana (KeywordPlus):  KAPPA
 [...]
 Avainsana (tekijät):  diabetic retinopathy
 Avainsana (tekijät):  diabetic maculopathy
 [...]
 Lähde: 0010603696 /  *DIAB CONTR COMPL /  NEW ENGL J MED /  977 
 /  329 /  1993
 Lähde: 0034118371 /  *DIAB CONTR COMPL /  ARCH OPHTHALMOL-CHI /  
 1344 /  105 /  1987
 Lähde: 0075276068 /  *DRS RES GROUP /  OPHTHALMOLOGY /  82 /  85 
 /  1978
 \end{verbatim}

\section{Analyzing textual data}
Basically the descriptions are natural English 
language appended with the citation references. When analyzing 
this kind of textual data the often used methods involve some 
kind of counting. We count for example which
are the most used words, which words appear together and so on.
% We can count n-grams.
This is called text analysis. One of the main principles was made 
popular by George Kingsley Zipf. He stated that the frequency of 
a word in any corpus is approximately inversely proportional to 
its rank. \fixme{This needs clarification/pruning}

Next we will describe the steps needed to prepocess the raw data
so that it can be analysed.
\subsection{Preprocessing}
\fixme{Explain stop words etc.}
We start by parsing the different metadata fields from the raw 
text records to a structured objects by recognizing the titles of 
the fields and related field descriptions.
% After parsing data is written to interim files.
The preprocessing for text analysis usually also 
includes the removing of \emph{the stop words} from data. Stop 
words are the most frequent words in the data like: \emph{``the, 
of, and, where etc.''} Because these words are present in any text 
they probably don't tell much about the topics these records that 
is about the scientific field of the data. \cite{ref_here}
\subsection{Lemmatizing}
After removing stop words the next step is to unify the 
differrent written forms a concept. We might have eg. ``visual'' 
and ``visually'' or ``dog'' and ``dogs'' and we want them as 
``visual'' and ``dog'' only.
There are two possible options to achieve this. 
\emph{Lemmatizing} means replacing each inflectional form of a 
word with its nominative form. This is done to reduce the number 
of possible features with multiple inflicted form of a word. 
[...] \cite{ref_here}. This is normally done against a dictionary. 
The same lemmatizing should also be done to the stop words before 
they are removed.

\emph{Stemming} means stripping the word of its termination such 
that only the stem of the word is retained. 


\subsection{Different distributions of different metadata fields}
\fixme{Pitikö tämä aliluku jättää pois?} There are multiple ways 
to use input metadata of the publications; put all text in one 
bin and use text analysis methods on that. Alternatively we could 
assume different distribution for each metadata field 
\fixme{describe metadata before this} and treat them separately.
\subsubsection{Keyword distribution}
\subsubsection{Citation distribution}
- Normalization of citation weight between different disciplines.
  Different fields of science have different citation practices 
eg. regarding the number of references per publication. 
\cite{waltman_new_2012}


\section{Dimensionality reduction}
\label{sec:dimensionalityreduction}


\subsection{Singular value decomposition}
Singular value decomposition (SVD) is a common method to reduce 
the dimensionality of data. 
\fixme{Vertaa/havainnollista miten eroaa PCA:sta}

\section{Agglomerative clustering}
\label{sec:agglomerativeclustering}
There are also multiple methods to use for the problem. LDA is one.

\subsection{Distance metric}
Data is high dimensional so choosing distance measure is 
important. The higher the number of dimensions the more similar 
distance each observation is from every other observation. This is
known as \emph{curse of dimensionality} \fixme{citation}.
Possible distance measures are:
- cosine angle (uncentered Pearson correlation)\fixme{look: 
\url{https://www.researchgate.net/post/What_is_the_best_distance_
measure_for_high_dimensional_data/4}}
-euclidean
-mahalnobis
Boyack et al. have compared clustering 
real world size corpus of 2.5 million publications from MEDLINE
with nine different similarity metrics \cite{boyack_clustering_2011}...

\subsection{Linkage methods}
In this work we use agglomerative hierarchical clustering with 
Ward's distance metric.\fixme{citation}

\subsubsection{Single linkage}
\subsubsection{Complete linkage}
\subsubsection{Average linkage}
\subsubsection{Ward's method}

\subsection{Complexity}
Time and space requirements of used agglomerative clustering method
are $O(N^2)$ for both.\fixme{citation}


