\chapter{Data and Methods}
\label{chapter:methods}
In this chapter we present the data and methods used in the 
clustering. We follow the logical order the methods are applied on 
the data.

\section{Publication metadata}
The data consits of X records of Clarivative Analytics' 
(formerly Thomson Reuters) Web of Science publication metadata 
from year Y.

Our data here consists of short metadata records describing the 
publications. An example of a shortened record:
\begin{verbatim}
 Lehti: ACTA OPHTHALMOLOGICA SCANDINAVICA
 ISSN: 1395-3907
 Ala: OPHTHALMOLOGY
 Ilmestymisvuosi:   1999
 Otsikko: Assessment of diabetic retinopathy using two-field 60 
 degrees fundus photography. A comparison between[...]
 Avainsana (KeywordPlus):  OPHTHALMOSCOPY
 Avainsana (KeywordPlus):  KAPPA
 [...]
 Avainsana (tekijät):  diabetic retinopathy
 Avainsana (tekijät):  diabetic maculopathy
 [...]
 Lähde: 0010603696 /  *DIAB CONTR COMPL /  NEW ENGL J MED /  977 
 /  329 /  1993
 Lähde: 0034118371 /  *DIAB CONTR COMPL /  ARCH OPHTHALMOL-CHI /  
 1344 /  105 /  1987
 Lähde: 0075276068 /  *DRS RES GROUP /  OPHTHALMOLOGY /  82 /  85 
 /  1978
 \end{verbatim}

 
\section{Analyzing textual data}
Basically the descriptions are natural English 
language appended with the citation references. When analyzing 
this kind of textual data the often used methods involve some 
kind of counting. We count for example which
are the most used words, which words appear together and so on.
% We can count n-grams.
This is called text analysis. One of the main principles was made 
popular by George Kingsley Zipf. He stated that the frequency of 
a word in any corpus is approximately inversely proportional to 
its rank. \fixme{This needs clarification/pruning}

Next we will describe the methods used in this work.


\subsection{Preprocessing}
The preprocessing for text analysis usually also includes the 
removing of \emph{the stop words} from data. Stop words are the 
most frequent words in the data like: \emph{``the, of, and, where 
etc.''} Because these words are present in any text they probably 
don't tell much about the topics these records concern. 
\cite{ref_here}


\subsection{Lemmatizing}
After removing stop words the next step is to unify the 
different written forms a concept. We might have eg. ``visual'' 
and ``visually'' or ``dog'' and ``dogs'' and we want them as 
``visual'' and ``dog'' only. This is desirable to reduce the 
redundant repetion the data and also to reduce the dimensions of 
the feature space. [...] \cite{ref_here}
There are two possible options to achieve this. 
\emph{Lemmatizing} means replacing each inflectional form of a 
word with its nominative form, or lemma. The context of the word 
is used to decide the correct lemma in the case of a word with 
multiple meanings. Nevertheless these homographic word forms can 
be problematic and cause ambiguity. The same lemmatizing should 
also be done to the stop words before they are removed.

\emph{Stemming} means stripping the word of its termination such 
that only the stem of the word is retained. No conext is used and 
only the word itself is inspected.

Lemmatization results in better precision, or true negative rate 
but poorer recall true positive rate compared to stemming. 
\cite{ref_here}


\subsection{Vectorizing}
The clustering algorithms don't understand text documents but 
require numerical input. To enable the handling of the textual 
data by the clustering algorithms we have to transform it into 
numerical form. This is called vectorizing. \fixme{Tämä 
seuraavaan implementaatiolukuun?}

\subsection{Topic modeling}
\fixme{Kuuluko tämä aihe mukaan?}
Yau et al. \cite{ref_here} have been researched application of 
topic modeling to bibliometrics.


\subsection{Different distributions of different metadata fields}
\fixme{Pitikö tämä aliluku jättää pois?} There are multiple ways 
to use input metadata of the publications; put all text in one 
bin and use text analysis methods on that. Alternatively we could 
assume different distribution for each metadata field 
\fixme{describe metadata before this} and treat them separately.
\subsubsection{Keyword distribution}
\subsubsection{Citation distribution}
- Normalization of citation weight between different disciplines.
  Different fields of science have different citation practices 
eg. regarding the number of references per publication. 
\cite{waltman_new_2012}


\section{Dimensionality reduction}
\label{sec:dimensionalityreduction}
The textual data is encoded with bag of words model, where 
meaning that for each word in the corpus, in this case almost 
whole English language, the record has a binary feature that is 1 
if that word occurs in the record and 0 if not, the resulting 
record-feature matrix has the dimensionality of $m*n$ where $m$ in 
the number of records and $n$ is the number of features and very 
sparse. In our case the record-feature matrix has the 
dimensionality approximately $10000*20000$. 


\subsection{Singular value decomposition}
Singular value decomposition (SVD) is a common method to reduce 
the dimensionality of data. 
\fixme{Vertaa/havainnollista miten eroaa PCA:sta}


\section{Agglomerative clustering}
\label{sec:agglomerativeclustering}
In this work we use agglomerative hierarchical clustering with 
Ward's distance metric.\cite{ref_here} Agglomerative clustering 
starts with each document as a cluster of its own and then 
merges pairs of clusters together as the clustering process goes 
on.


\subsection{Distance metric}
Our data is high-dimensional so choosing distance measure is 
important. The higher the number of dimensions the more 
dissimilar each observation seems to from every other observation. 
This is known as \emph{curse of dimensionality} \fixme{citation}.
Boyack et al. have compared clustering real world size corpus of 
2.5 million publications from MEDLINE with nine different 
similarity metrics. \cite{boyack_clustering_2011}
Next we will briefly present some distance measures.

Cosine angle is invarian(uncentered Pearson 
correlation)\fixme{look: 
\url{https://www.researchgate.net/post/What_is_the_best_distance_
measure_for_high_dimensional_data/4}}

Euclidean distance is the familiar distance used in everyday life:
\begin{equation}
 ||a-b||_2 = \sqrt{\sum_i{(a_i-b_i)^2}}
\end{equation}

Mahalnobis distance...

We use Manhattan distance because it works well with sparse data.
\cite{ref_here}
\begin{equation}
 ||a-b||_1 = \sum_i{(a_i-b_i)}
\end{equation}


\subsection{Linkage methods}
Linkage methods affects how clusters are formed and how the 
distance metric is applied between two clusters.

\subsubsection{Single linkage}
Single linkage defines that the distance between two clusters is 
measured as distance between two closest items of the clusters 
$A$ and $B$.
\begin{equation}
 min\{d(a,b):a \in A, b \in B\}
\end{equation}

\subsubsection{Complete linkage}
Complete linkage is defined as the distance between the most 
distant items of clusters $A$ and $B$.
\begin{equation}
 max\{d(a,b):a \in A, b \in B\}
\end{equation}

\subsubsection{Average linkage}
Average linkage is defined as the average of the all pairwise 
distances between the items of cluster $A$ and $B$.
\begin{equation}
 \frac{1}{|A| \cdot |B|} \sum_{a \in A} \sum_{b \in B}d(a,b)
\end{equation}

\subsubsection{Ward's method}
% first 'ref_here': 
% www.statisticshowto.datasciencecentral.com/wards-method
We are using Ward's method because it usually creates compact 
even-sized clusters. \cite{ref_here} Ward's method minimizes the 
total within-cluster variance.
\begin{equation}
 definition\ here
\end{equation}



\subsection{Complexity}
Time and space requirements of used agglomerative clustering method
are $O(N^2)$ for both.\fixme{citation}



