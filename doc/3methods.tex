\chapter{Data and Methods}
\label{chapter:methods}
In this chapter we present the data and methods used in the 
clustering. We follow the logical order in which the methods are 
applied on the data. We first describe the raw data, 
pre-processing it, the feature extraction transforming irregular
length symbolic data (text) into numerical vector representation,
reducing the dimensionality of the data, selecting the model used 
to learn from the data and finally the selected clustering method.

\section{Publication metadata}
\label{section:metadata}
The data consists of $10456$ records of Clarivate's Web of Science 
publication metadata from year 2000. Each record describes some 
basic information about an article published in a scientific 
journal. The data contains only publications with at least one
author with an affiliation to a Finnish research organisation as
recorded in the publication. These publications were published in 
total in $2539$ different scientific journals.
An example of a shortened record:
\begin{verbatim}
 Lehti: ACTA OPHTHALMOLOGICA SCANDINAVICA
 ISSN: 1395-3907
 Ala: OPHTHALMOLOGY
 Ilmestymisvuosi:   1999
 Otsikko: Assessment of diabetic retinopathy using two-field 60 
 degrees fundus photography. A comparison between[...]
 Abstrakti
 Purpose: To assess the severity of diabetic retinopathy and 
 maculopathy bycomparing[...]
 Avainsana (KeywordPlus):  OPHTHALMOSCOPY
 Avainsana (KeywordPlus):  KAPPA
 [...]
 Avainsana (tekijät):  diabetic retinopathy
 Avainsana (tekijät):  diabetic maculopathy
 [...]
 Lähde: 0010603696 /  *DIAB CONTR COMPL /  NEW ENGL J MED /  977 
 /  329 /  1993
 Lähde: 0034118371 /  *DIAB CONTR COMPL /  ARCH OPHTHALMOL-CHI /  
 1344 /  105 /  1987
 Lähde: 0075276068 /  *DRS RES GROUP /  OPHTHALMOLOGY /  82 /  85 
 /  1978
 \end{verbatim}
 
Each metadata record contains a single instance of the title 
(``\texttt{Lehti}''), ISSN (International Standard Serial Number), 
the field of science (``\texttt{Ala}'') and publication year 
(``\texttt{Ilmestymisvuosi}'') of the journal, the title 
(``\texttt{Otsikko}'') and the abstract (``\texttt{Abstrakti}'') 
of the article. Additionally each metadata record can contain
multiple instances of keywords inferred by the publisher programmatically
(``\texttt{Avainsana (KeywordPlus)}''), keywords produced by the 
authors themselves (``\texttt{Avainsana (tekijät)}'') and the 
cited references (``\texttt{Lähde}''). The data contains incomplete,
erroneous and multiplicated records.

% \fixme{Tarkista YL:ltä että lähteet ovat nimenomaan 
% artikkelin omat. (Viitteet voivat olla metodologisia, 
% historiallisia ja siten johtaa harhaan). Toisaalta en nyt käytä 
% työssäni viitteitä (keskustelu LM:n kanssa 20.9.).} OK


 
\section{Feature extraction}
The clustering algorithms don't understand text documents but 
require numerical input. To enable the handling of the textual 
data by the clustering algorithms we have to transform it into a
numerical form. We usually describe the data as a matrix where 
each \emph{data sample}, or observation, a single publication in 
our case, is one row in the matrix. Each data sample consists of
\emph{features}, that is numerical values representing some aspect
of the sample. So it follows that the rows of the matrix are 
feature vectors of the data samples and the columns are the 
individual features. Feature extraction is the process used to 
transform text documents into feature vectors (ie. data samples).
The number columns in the data matrix corresponds to the order of 
the feature space of the task.

\subsection{Analyzing textual data}
The descriptions are natural English language appended with the 
citation references. When analyzing 
this kind of textual data the often used methods involve some 
kind of counting. We count, for example, to find the most used 
words in a document, which words appear together and so on.
% We can count n-grams.
% This is called text analysis. 
Next we will describe the methods used in this work.


\subsection{Preprocessing}
The preprocessing for text analysis usually also includes the 
removing of \emph{the stop words} from data. Stop words are the 
most frequent words in the data like: \emph{``the, of, and, where 
etc.''} Because these words are present in any text they probably 
don't tell much about the topics these records concern as 
presented by Luhn \cite{luhn_key_1960}


\subsection{Lemmatisation}
After removing stop words the next step is to unify the 
different written forms a term. We might have eg. ``visual'' 
and ``visually'' or ``dog'' and ``dogs'' and we want them as 
``visual'' and ``dog'' only. This is desirable to reduce the 
redundant repetion of the data and also to reduce the dimensionality of 
the feature space. \cite{siemens_lemmatization_1996} 
\cite{hann_towards_1975}
There are two possible options to achieve this. 

\emph{Lemmatisation} means replacing each inflectional form of a 
word with its nominative (ie. dictionary) form, or lemma. 
The problem is the ambigousness of many natural words. To 
achieve lemmatisation, many tactics from simple dictionary look-ups to 
rule-based systems, to sophisticated algorithms to infer the role 
of the word in the sentence as well as using the larger context are
employed. The stop words should be in their nominative form so they 
should be removed after lemmatization.
% The same lemmatizing should also be done to the stop words before they are 
% removed.

\emph{Stemming} means stripping the word of its termination such 
that only the stem of the word is retained. No context is used 
and only the word itself is inspected. It is much simpler procedure
compared to lemmatisation.

Lemmatization results in better precision, or true negative rate 
but poorer recall true positive rate compared to stemming. 
\cite{manning_introduction_2008}


\subsection{Vectorisation}
After lemmatising the terms we count the occurences of each term 
in each document. This is called \emph{the bag of words} 
representation of the textual data because for each word in the 
corpus, we only count it ignoring all its positional information 
in relation to other words. So each document is represented by
a vector of its term frequencies. This vector is called \emph{the 
feature vector} of the document. 
% This is called vectorising.
% When vectorising whole data set
The resulting term occurence 
frequencies are normalized to decrease the importance of the 
tokens that occur in the majority of documents. Usually these are 
common terms not specific to the topic of the document. 
These normalized occurence frequencies are called term 
frequency inverse document frequencies, TF-IDF, and 
they form the features of a document. The size of the feature 
space is determined by the number of counted unique terms in all 
documents of the collection. 
TF-IDF is defined as follows:

\begin{equation}
 TF-IDF(t,d) = tf(t,d) \times log \frac{1+n}{1+df(t)} + 1,
\end{equation}

where $tf(t,d)$ is the term frequency, count of a term $t$ in a 
document $d$, $n$ is the number of documents in the data set and 
$df(t)$ the number of documents containing term $t$ 
\cite{luhn_statistical_1957} \cite{jones_statistical_1972}.

Here the size of the feature space is in the order of the the 
size of the English language corpus ie. in tens of thousands. The 
document feature matrix is very sparse, a single document only 
having handful of terms of the whole corpus.
% 29.4.2020 Vain prosenttiosuus tiheydestä.  Ei ehdi nyt.



\section{Dimensionality reduction}
\label{sec:dimensionalityreduction}
The resulting publication-feature matrix has the dimensionality 
of $m \times n$ where $m$ in 
the number of records and $n$ is the number of features and very 
sparse. In our case the record-feature matrix has the 
dimensionality of approximately $10000 \times 20000$. 
So the dimensionality of our problem, number of features, is very
high compared to the number of samples. This means that all
samples are very sparsely and very far away from each other in the 
feature space.
The higher the number of dimensions the more 
dissimilar each observation seems from every other observation. 
This is known as \emph{the curse of dimensionality} \cite{trunk_problem_1979}.
To tackle this, and also to fasten the computation of the 
clustering we have to reduce the dimensionality of the feature space.


\subsection{Singular value decomposition}
In text analysis setting the logical reasoning for dimensionality
reduction is known as Latent semantic analysis (LSA)\cite{dumais_using_1988}.
LSA leads to a linear algebra tool called singular value 
decomposition (SVD). SVD is a common method to reduce the 
dimensionality of data. SVD for a matrix $M$ is defined as:

\begin{equation}
 M = U \Sigma V^T
\end{equation}

$U$ is unitary matrix, $\Sigma$ is the diagonal matrix of singular
values $\sigma_i$ and $V$ consists of orthonormal set of singular 
vectors of $M$.

SVD transforms data lineary into ``a new feature space'' in which the 
features are not correlated. In the decomposition the new features are 
ordered in decreasing order of variance so the least significant 
ones can be omitted. 
Another commonly used method, principal component analysis (PCA), 
achieves the same.
Whereas PCA requires the data matrix to be square and have 
eigendecomposition, SVD does not, but achieves the same goal.
So it is in a way another algebraic solution to that.


\section{Model selection}
Generally all machine learning problems are ill-posed in the sense 
that a unique solution for the problem can't be found unless some
assumptions, or \emph{inductive bias}, are introduced. This begins 
with selecting the learning algorithm and might also include some
hyperparameters of selected algorithm.

Here we will use unsupervised learning to shape the mapping of 
scientific disciplines because we want to learn the possible 
intristic structure of sciencetific knowledge.  
There are many different clustering algorithms from which to 
choose. Some often used common algorithms are k-means, hierarchical
clustering, density based scan clustering and Gaussian clustering.
We will use hierarchical clustering because it's quite simple and 
familiar to us.
In hierarchical clustering there are yet different parameters to 
choose. 
% We will handle those in later chapters.
Hierarchical clustering with single, average and complete linkages
and Ward's method applied to search query result clustering were 
studied by Korenius et al. \cite{korenius_hierarchical_2006}.

\subsection{Choosing the number of clusters}
The number of clusters $k$ that should result from the clustering is
not known beforehand but is a required parameter of the
clustering algorithm. Hierarchical clustering will always return 
$k$ clusters regardless if they are meaningful or not. To make a
decision about the number of clusters we can 1) inspect the data in 
two dimensions using some dimensionality reduction method such as 
PCA and then plot it, 2) set limits for the inter-cluster 
distances at each step of merging clusters ie. if finding the 
largest gap between the dendrogram levels or 3) measuring the 
overall compactness of clusters and their separation from each 
other over the number of clusters $k$ with suitable evaluation
criterion such as average Silhouette value 
\cite{alpaydin2004introduction} \cite{calinski_dendrite_1974} 
\cite{rousseeuw_silhouettes:_1987}. The first method is assumed to 
give uninformative view of the data. We will use the third method.
% Toinen vaihtoehto mahdollisesti samankaltainen kolmannen kanssa?





% TODO: Kirjoita nämä jonnekin näille main tai poista
% We need a method to measure the ``goodness'' of the clustering.
% As we work with text data, the dimensionality increases
% quite high and projecting data down to 2 or 3 dimensions for 
% visualization is not a simple task. (We come back to visualization 
% later though.)
% % LM 06/20: perustelut hyvin asian ongelmallisuudelle
% So we have to resort to measurements derived from the 
% resulting clustering itself. If we knew some underlying ground 
% truth behind our clustering problem, we could validate our result 
% against it. But as mentioned earlier, even defining what actually 
% are the current fields of science depends on who you ask and for 
% what purpose the definition is needed. So the ground truth is only
% one measure for our results here.
% In the lack of ground truth we can use some ``internal'' goodness 
% measure for the resulting clustering. These kind of measures 
% basically try to infer how dense the clusters are compared with how 
% sparse the inter-cluster space is and how well the clusters are 
% separated from each other.
% 
%\section{Silhouettes}
% One such measure to estimate the ``goodness'' of a clustering is 
% silhouettes. Silhouettes use average proximities that are know
% to work best with clear, compact and spherical clusters
% \cite{rousseeuw_silhouettes:_1987}. Silhouette value for an item
% is defined as:






\subsubsection{Internal validation of clustering results}
Liu et al. have reviewed 11 commmonly used internal clustering 
validation indices \cite{liu_understanding_2010}. These are used 
to decide the correct number of clusters. We will use two 
of those; Calinski-Harabasz criterion \cite{calinski_dendrite_1974} 
and Silhouette value \cite{rousseeuw_silhouettes:_1987}. We choose 
these because they were familiar for us and have different
weak spots as noted by Liu et al. and thus could complement each 
other. 

\subsubsection{Calinski-Harabasz criterion}
Calinski-Harabasz criterion is defined as
\begin{equation}
 CH = \frac{SS_B}{N-k} \frac{SS_W}{k-1},
\end{equation}
where $SS_B$ is \emph{the between-group sum of squares} that gives
the overall variance between clusters
\begin{equation}
 SS_B = \sum_{i=1}^k n_i ||m_i-m||^2,
\end{equation}
and $SS_W$ is \emph{the within-group sum of squares} that gives 
the overall variance within clusters
\begin{equation}
 SS_W = \sum_{i=1}^k \sum_{x\in C_i} ||x-m_i||^2.
\end{equation}
In equations above $k$ is the number of clusters, $N$ is the 
number of observations, $n_i$ is the number of observations in 
cluster $i$, $m_i$ is the centroid of cluster $i$, $m$ is the 
overall mean of sample data, $x$ is the observation, $C_i$ is the 
$i$th cluster and $|| \cdot ||$ is the Euclidean 
distance between the two vectors. The larger the 
Calinski-Harabasz criterion, the better the cluster structure.

\subsubsection{Silhouette value}
Silhouette value of a single clustered observation $x$ is defined 
as follows.
\begin{equation}
 S(x) = \frac{1}{k}\sum_i{\frac{1}{n_i}\sum_{x \in C_i}\frac{b(x) - a(x)}{max(b(x), a(x))},
\end{equation}
where $a(x)$ average dissimilarity of the observation $x$ to all 
other observations in its own cluster 
$C_i$:
\begin{equation}
  a(x) = \frac{1}{n_i - 1} \sum_{y \in C_i, y \neq x}d(x,y)  
\end{equation}
and $b(x)$ is the average dissimilarity of the observation $x$ to 
all observations $y$ in the next closest cluster $C_j$:
\begin{equation}
  b(x) = min_{j,j \neq i}[\frac{1}{n_j} \sum_{y \in C_j}d(x,y)]
\end{equation}
In equations above $k$ is the number of clusters, $n_i$ is the 
number of observations in cluster $i$, $x$ is the observation, 
$C_i$ is the $i$th cluster and $d(x,y)$ is some dissimilarity 
measure on ratio scale (eg. Euclidean distance) between the two 
vectors. Silhouette values are bounded to range $[-1,1]$. Values
close to $1$ mean that the observation $x$ is very well clustered 
in its cluster, value $0$ means that the observation could as well
be clustered in the nearest neighbor cluster and values close to 
$-1$ mean that the observation is almost certainly in the wrong 
cluster.

\subsubsection{External validation of clustering results}
\label{sec:ext_val}
% Otsikko oli: Manually annotated validation set
% Gold standard set. Actually a gold standard set would be a set
% of all data sets with abstract excluding sets that don't have it.
% Tavoite
To support the choise of internal validation method for choosing
the number of clusters, we will use the external validation 
with a manually annotated data set. 
% as a measure of best internal validation method.
Goal will be to achieve evaluation measurements based on a quite 
clearly separated set of publications. 
% More vaguely classifiable publications were included for comparison.
% For not to have to rely solely on internal clustering validation we
% also try external clustering validation with small subset of data.
We will create a manually annotated validation set for calculating
precision, recall and other metrics derived from those.
The validation set will consist of publications from three different
discipline. We will experiment which of the internal validation
methods, Silhouette value or Calinski-Harabasz criterion, will
indicate more clearly the known, correct number of clusters.

\subsubsection{Adujsted Rand index}
We will compare the internal validation result with Adjusted Rand 
index (ARI) \cite{hubert_comparing_1985} which takes into account 
our defined correct clustering. Adjusted Rand index is defined as 
follows.
\begin{equation}
 ARI = \frac{RI - E[RI]}{max(RI) - E[RI]},
\end{equation}
where $RI$ is the ``raw'' Rand index and $E[RI]$ its expected
value. The Rand index is defined then as:
\begin{equation}
  RI = \frac{TP + TN}{TP + FP + FN + TN},
\end{equation}
where $TP$ is the number of true positives, $TN$ true negatives, 
$FP$ false positives and $FN$ false negatives.

\subsubsection{Precision and recall}
% määrittele peruskäsitteet kuten precision ja recall
Recall is the number of items correctly classified divided by the 
total number of items in that class.
\begin{equation}
 Recall = \frac{|\{relevant\ items\} \cap \{retrieved\ items\}|} 
{\{relevant\ items\}}
\end{equation}

Precision is the number of items correctly classified divided by 
the total number of items classified into that class (ie. true and 
false positives).
\begin{equation}
 Precision = \frac{|\{relevant\ items\} \cap \{retrieved\ 
items\}|} 
{\{retrieved\ items\}}
\end{equation}


\section{Agglomerative clustering}
\label{sec:agglomerativeclustering}
In this work we use agglomerative hierarchical clustering with 
Ward's distance metric \cite{ward_jr_hierarchical_1963}. This 
method was chosen because it was familiar to us is known to 
perform well enough in simple settings. 
% \fixme{Miksi, perustelut?} 
Agglomerative clustering starts with each document 
as a cluster of its own and then merges pairs of clusters together 
as the clustering process goes on.


\subsection{Distance metric}
Our data is high-dimensional so choosing distance measure is 
important. 
% The higher the number of dimensions the more 
% dissimilar each observation seems from every other observation. 
% This is known as \emph{the curse of dimensionality} \cite{trunk_problem_1979}.
Boyack et al. have compared clustering real world size corpus of 
2.5 million publications from MEDLINE with nine different 
similarity metrics \cite{boyack_clustering_2011}.
% Next we will briefly present some distance measures.
% Cosine angle is invariant (uncentered Pearson 
% correlation)\fixme{look: 
% \url{https://www.researchgate.net/post/What_is_the_best_distance_
% measure_for_high_dimensional_data/4}}
For example Manhattan distance works well with sparse data.
% \cite{ref_here}
\begin{equation}
 ||a-b||_1 = \sum_i{(a_i-b_i)}
\end{equation}
Euclidean distance is the familiar distance used in everyday life:
\begin{equation}
 ||a-b||_2 = \sqrt{\sum_i{(a_i-b_i)^2}}
\end{equation}
Because Ward's method assumes it, we use it.

% Mahalnobis distance...


\subsection{Linkage methods}

% Vain mitä aion käyttää!
% Menetelmien kuvaus - mistä siinä on kyse!?!

Linkage methods affects how clusters are formed and how the 
distance metric is applied between two clusters.

\subsubsection{Single linkage}
Single linkage defines that the distance between two clusters is 
measured as distance between two closest items of the clusters 
$A$ and $B$.
\begin{equation}
 min\{d(a,b):a \in A, b \in B\}
\end{equation}

\subsubsection{Complete linkage}
Complete linkage is defined as the distance between the most 
distant items of clusters $A$ and $B$.
\begin{equation}
 max\{d(a,b):a \in A, b \in B\}
\end{equation}

\subsubsection{Average linkage}
Average linkage is defined as the average of the all pairwise 
distances between the items of cluster $A$ and $B$.
\begin{equation}
 \frac{1}{|A| \cdot |B|} \sum_{a \in A} \sum_{b \in B}d(a,b)
\end{equation}

\subsubsection{Ward's method}
% first 'ref_here': 
% www.statisticshowto.datasciencecentral.com/wards-method
We are using Ward's method because it usually creates compact 
even-sized clusters \cite{strauss_generalising_2017}. Ward's 
method minimizes the total within-cluster variance for merging 
the next possible clusters $A$ and $B$:
\begin{equation}
 I_{AB} = \frac{n_A n_B}{n_A + n_B} (\bar{\textbf{a}} - \bar{\textbf{b}})'(\bar{\textbf{a}}-\bar{\textbf{b}})
\end{equation}
where $\bar{\textbf{a}}$ and $\bar{\textbf{b}}$ are the centroids
of cluster $A$ and $B$ respectively and $n_A$ and $n_B$ are
corresponding number of members in clusters $A$ and $B$.


\subsection{Algorithmic complexity}
Time complexity of the used agglomerative clustering with Ward's 
method is $O(N^3)$ and space complexity is $O(N^2)$, 
where $N$ is the number of publications 
\cite{willett_recent_1988}. These are quite demanding requirements
when the number of publications increase. In this case it is 
acceptable because there is no need for real time updates of 
results, rather we experiment with exploratory data analysis.



