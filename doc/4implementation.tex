\chapter{Implementation}
\label{chapter:implementation}

% Miten ratkaisin ongelman?
% Arkkitehtuuri
% Työvoita
% Teknologia
% Eteen tulleet ongelmat toteutuksessa
% EI ohjelmadokumentti!

Here we describe how the methods are implemented to achieve the 
clustering. First we describe how the raw text data is 
pre-processed 

We implemented the workflow for clustering using Python's 
\texttt{scikit-learn} package completed with 
pre-processing managed with \texttt{doit} workflow. See Figure 
\ref{fig:wf} for the workflow graph.

\begin{figure}[ht]
  \begin{center}    
    \input{images/workflow.tex}
    \caption{The workflow of the clustering.}
    \label{fig:wf}
  \end{center}
\end{figure}

\section{Preprocessing}
First data was read from raw files obtained from publishers. 
%(Actually YL probably also preprocessed data before passing it 
% to me.)
We choose to omit some metadata fields from the analysis (cf. 
section \ref{section:metadata}). Fields \emph{journal}, 
\emph{issn} and \emph{the field of science} were omitted because 
they exhibit the existing classification whereas we wanted an 
alternative one. The field \emph{year} is irrelevant here as our 
analysis doesn't include temporal aspect. The field 
\emph{references} was also left out because the references 
would've needed different treatment and more theoretical research 
on how to be used as features for clustering \fixme{Täydennä 
perustelua}.

We choose to keep \emph{title}, \emph{abstract} and 
\emph{keyword} fields. All the title, abstract and author 
provided keywords are original data of the publication without 
any external interpretation. We also choose to keep the automatic 
publisher inferred keywords (``\texttt{Avainsana 
(KeywordPlus)}'') because these can be seen as concentration of 
publication data by some text analysis algorithm. The algorithm is 
unknown here but we assume that the aim of the these keywords is 
also to describe the publication as well as possible. 
\fixme{Paranna perustelua} Terms in keyword fields were 
concatenated into combined terms: 
``allergic\_contact\_dermatitis''. This is related to the counting 
of words in a metadata and will be explained soon. The kept 
metadata fields were read into a dictionary and data saved in 
intermediate files.

\section{Tokenizing}
The metadata in chosen fields was then tokenized with whitespace 
and punctuation other than perioids separating the tokens (words). 
We get for example ``\emph{Pluto is a smart dog}'' $\rightarrow$ 
``\emph{Pluto}'', ``\emph{is}'', ``\emph{a}'', ``\emph{smart}'' 
and ``\emph{dog}''. This strategy is known as \emph{Bag of Words} 
representation because each word is taken as is 
ignoring all its positional information in relation to other 
words. 
Alternatively we could form n-grams out of the text to preserve 
the meaning of combined terms. For example when tokenizing 
``\emph{allergic contact dermatitis}'', instead of 
``\emph{allergic}'', ``\emph{contact}'' and 
``\emph{dermatitis}'' we would get ``\emph{allergic contact}'' 
and ``\emph{contact dermatitis}''. Here single words are called 
1-grams, word pairs are called 2-grams and so on. Counting the 
n-grams would expand the feature space n-fold though and we wanted 
to avoid the computational cost. \fixme{Koitetaan ottaa 2-grammit 
mukaan}

\section{Lemmatizing}
After getting the words we lemmatized them against WordNet 
lemmatizer.
% After parsing data is written to interim files.
We choose to remove all plain numbers from
data. Another option is to replace numbers with dummy '\#NUMBER'.
That could help separate natural sciences from humanities. Because 
we didn't have proper knowledge on the issue removal was chosen. So 
numbers would be ignored. That would be a more neutral way to treat 
them. \fixme{How? Explain...}
If the lemmatizer didn't found the word in WordNet Lemmatizer 
database the word is returned unchanged. \fixme{Huomioita 
lemmatisoinnin tuloksesta}

\section{Removing stop words}
Stop words had to be lemmatized also before data could be 
filtered to remove those. We used NLTK modules English stop words 
combined with \texttt{scikit-learn's} stop words and our own stop 
word list consisting mainly standard publisher notices 
(\emph{``rights'', ``reserved''} and publising years). The stop 
word list included 352 words.


\section{Vectorizing}
After removing stop words the term frequencies were counted and 
normalized with inverse document frequencies (\emph{tf-idf} 
normalization). In this point we have the data vectorized.

We set the minimum document frequency, under which the terms 
having their \emph{tf-idf} value are ignored, to $min_df=2$, so 
two occurences. The maximum document frequency, over which the 
terms having their \emph{tf-idf} value are ignored, to 
$max_df=0.5$, so to 50 \% of the documents.
These cut-off values resulted in 7123 ignored terms and the 
feature space of 3372 terms. Terms that were filtered out as too 
frequent or too few were for example: ``haemoglobin\_a'', 
``jacobian\_matrix'', 
``parthenogenetic'', ``chelation\_by\_saccharide\_molecules'', 
``leg\_blood\_supply'', ``computational\_fluid\_dynamics'', 
``pigment'', ``hdtv'', ``nicotinic\_receptor''.

\fixme{To verify these parameter values it would be interesting 
to know the frequencies of the filtered-out terms. It didn't come 
out straight from TfIdfVectroizer class used but taking different 
levels of $min_df$ and $max_df$ could be used to view different 
frequency groups.}

\fixme{Onko näissä esikäsittelyvaiheissa joitain parametreja 
jonka suhteen tuloksia haluttaisiin tarkastella? Riittänee että 
tokenointi ja lemmatisointi toimivat. Noissa sanatiheyden
leikkausarvoissa min ja max-df voisi olla jotain.}


\section{Clustering}
When clustering with Ward's hierarchical clustering the 
interesting parameter values are the number of the clusters, 
the distance measure, the cluster connectivity measure, the 
number of the components ie. the amount of dimensionality 
reduction.



