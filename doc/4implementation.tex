\chapter{Implementation}
\label{chapter:implementation}

% Miten ratkaisin ongelman?
% Arkkitehtuuri
% Työvoita
% Teknologia
% Eteen tulleet ongelmat toteutuksessa
% EI ohjelmadokumentti!

We first describe briefly the used programming language and 
environment and then how the methods are implemented to achieve the 
clustering. 

We implemented the whole workflow with Python 3 programming 
language. The core clustering and performance evaluation modules
were imported from Python's \texttt{scikit-learn} package 
\cite{scikit-learn}. \texttt{scikit-learn} is a widely used open 
source package for machine learning, providing functions for 
classification, regression, clustering and pre-processing amongst 
other things. It is openly developed by international community of
developers. We wrote functions for pre-processing, feature 
extraction, clustering and plotting around these modules. 
The top level workflow management was written using Python's 
\texttt{doit} workflow management package. In total this work 
amounted roughly 1500 lines of functioning Python code, 
approximately half of which is our contribution and the rest from 
open source documentation. All code is publicly 
available\footnote{https://github.com/juholehtonen/clustering}. 
Due to licensing agreements between Ministry of Education and 
Culture and Clarivate the data to reproduce the results is not 
publicly available. The overall workflow is depicted in Figure 
\ref{fig:wf}.
\begin{figure}[ht]
  \begin{center}    
    \input{images/workflow.tex}
    \caption{The workflow of the clustering.}
    \label{fig:wf}
  \end{center}
\end{figure}
The analysis was run on laptop environment with Fedora 28 Linux 
5.0.16-100.fc28.x86\_64, Intel Core i7-4800MQ CPU @ 2.70GHz, 16 GB
of RAM.

\section{Preprocessing}
\label{sec:impl_preproc}
The data was first read from raw files obtained from publishers. 
%(Actually YL probably also preprocessed data before passing it 
% to me.)
We choose to omit some meta data fields from the analysis (cf. 
section \ref{section:metadata}). Fields \emph{journal}, 
\emph{issn} and \emph{the field of science} were omitted because 
they exhibit the existing classification whereas we wanted an 
alternative one. The field \emph{year} is irrelevant here as our 
analysis doesn't include temporal aspect. The field 
\emph{references} was also left out because the references 
would have needed different treatment and more theoretical research 
on how to be used as features for clustering.
% \fixme{Täydennä perustelua}.

We choose to keep \emph{title}, \emph{abstract} and 
\emph{keyword} fields. All the title, abstract and author 
provided keywords are original data of the publication without 
any external interpretation. We also choose to keep the automatic 
publisher inferred keywords (``\texttt{Avainsana 
(KeywordPlus)}'') because these can be seen as concentration of 
publication data by some text analysis algorithm. The algorithm is 
unknown here but we assume that the aim of the these keywords is 
also to describe the publication as well as possible. 
% \fixme{Paranna perustelua} 
Terms in keyword fields were concatenated into combined terms: 
``allergic\_contact\_dermatitis''. This is related to the counting 
of words in a meta data and will be explained soon. The kept 
meta data fields were read into a dictionary and data saved in 
intermediate files.

\section{Tokenizing}
The meta data in chosen fields was then tokenized with whitespace 
and punctuation other than periods separating the tokens (words). 
We get for example ``\emph{Pluto is a smart dog}'' $\rightarrow$ 
``\emph{Pluto}'', ``\emph{is}'', ``\emph{a}'', ``\emph{smart}'' 
and ``\emph{dog}'' (the \emph{bag of words} representation).
We made an exception in tokenizing the keywords as explained above.
Alternatively we could form n-grams out of the text to preserve 
the meaning of combined terms. For example when tokenizing 
``\emph{allergic contact dermatitis}'', instead of 
``\emph{allergic}'', ``\emph{contact}'' and 
``\emph{dermatitis}'' we would get ``\emph{allergic contact}'' 
and ``\emph{contact dermatitis}''. Here single words are called 
1-grams, word pairs are called 2-grams and so on. Counting the 
n-grams would expand the feature space n-fold though and we wanted 
to avoid the computational cost. 
% Koitetaan ottaa 2-grammit mukaan
% 29.4. 2-grammit 'future work' -osioon

\section{Lemmatising}
After tokenizing the words we lemmatised them with 
NLTK's\footnote{https://www.nltk.org/, a Python package for natural language processing}
WordNet lemmatiser. The lemmatiser uses WordNet lexical database 
developed and maintained by Princeton University 
\cite{noauthor_princeton_2010}. It is a large general purpose 
English\footnote{Other languages are supported as well.} language 
database with over $155000$ words and over $175000$ synonym 
groups.
% After parsing data is written to interim files.
We choose to remove all plain numbers from
data. Another option is to replace numbers with dummy '\#NUMBER'.
That could help separate natural sciences from humanities. Because 
we did not have proper knowledge on the issue, removal was chosen. 
We assumed that to be a more neutral way to treat them. 
% \fixme{How? Explain...}
If the lemmatiser did not find the word in WordNet Lemmatiser 
database, the word was returned unchanged. 
% \fixme{Huomioita lemmatisoinnin tuloksesta}

\section{Removing stop words}
Stop words were then removed after lemmatisation. We used Python's
\texttt{NLTK} module's English stop words combined with 
\texttt{scikit-learn's} stop words and our own short stop word 
list consisting standard publisher notices: (\emph{``rights'', 
``reserved'', ``science'', ``elsevier'', ``2000''}). The stop word 
list included $383$ words.


\section{Vectorizing}
After removing stop words the term frequencies were counted and 
normalized with inverse document frequencies (\emph{tf-idf} 
normalization). 
In this point we have the data vectorized.

We set the minimum document frequency, under which the terms 
having their \emph{tf-idf} value are ignored, to $min_df=2$. So 
terms that have less than two occurrences are ignored. The maximum 
document frequency is set to $max_df=0.1$. So to terms 
occurring in more than 10 \% of the documents are also ignored.
% \fixme{To verify these parameter values it would be interesting 
% to know the frequencies of the filtered-out terms. It didn't come 
% out straight from TfIdfVectroizer class used but taking different 
% levels of $min_df$ and $max_df$ could be used to view different 
% frequency groups.} Ei ehdi

%\fixme{Onko näissä esikäsittelyvaiheissa joitain parametreja 
%jonka suhteen tuloksia haluttaisiin tarkastella? Riittänee että 
%tokenointi ja lemmatisointi toimivat. Noissa sanatiheyden
%leikkausarvoissa min ja max-df voisi olla jotain.}


\section{Clustering}
\label{sec:4clustering}
When clustering with Ward's hierarchical clustering the 
interesting parameter values are the number of the clusters, 
the distance measure, the cluster connectivity measure and the 
number of the components or the amount of dimensionality 
reduction. 

To decide the number of clusters we experimented with the manually 
annotated data set described in section \ref{sec:ext_val}.
% Käsin luokitellun aineiston kuvaus
The validation set consists of Finnish publications from year 2000
from three different fields of science as categorized by WoS.
Two of the fields are closely related sub fields of computer
science: \emph{computer science: information systems} (CS-IS) and 
\emph{computer science: 
artificial intelligence} (CS-AI), while the third one, 
\emph{clinical neurology}, is more distant from those. There are 
$116$, $122$, $250$ publications labelled as belonging to fields 
CS-IS, CS-AI and clinical neurology respectively, and $31$ 
publications labelled as belonging to both CS-IS and CS-AI, 
totalling $519$ publications in our data.

% Huonojen näytteiden poisto
Publications were inspected by title, abstract, keywords, journal
and publisher assigned disciplines of the journal. Publications
with critically missing data, unclear discipline assignment and
more like very application specific topics (e.g. a publication 
describing the development of a virtual community and a community
game platform) were excluded from validation set. 
% Luokittelun varmistus käsin
We then checked with layman's reasoning if the WoS labelled 
discipline seemed plausible. Because of the journal based 
classification, most publications had more than one assigned 
discipline. Only the previously named three discipline labels were 
retained because we wanted to test if and how well our clustering 
method would cluster the samples compared to these three 
groups. We asked an another opinion for 37 the most ambiguous 
publications that could have been in more than one of our 
categories. The second opinion was given by an academically educated 
person who was not an expert of all the fields in the data. 
Decision for these was made on consensus or by draw if needed.
For manually curated
validation set with discipline assignment and justifications for
possible exclusion see appendix \ref{chapter:first-appendix}.
% ``Väärin luokiteltujen uudelleen luokittelu''
In total $29$ publications were manually reclassified by either
choosing only one of two used classes or by assigning the 
publication to more fitting category by our judgement. After
excluding bad data and reclassifications we had a data set of 
$134$, $127$ and $194$ publications in CS-IS, CS-AI and clinical
neurology, respectively, totalling $455$ publications.

% Pitäisikö tämä olla jossain muualla?
% ``Väärin'' luokiteltujen uudelleen luokittelusta
The basic problem is that fields of science can not be 
defined so that they clearly differ from each other. Where one 
discipline end the other has already started like metallurgy and 
material science. Likewise, there are lots of publications that 
handle topics belonging to more than one discipline, e.g. this 
thesis discusses clustering and bibliometrics. So when annotating 
publications, deciding if a publication belonged to
a discipline or not felt often quite difficult. Often the 
separation between disciplines felt quite arbitrary. For example
an article describing using wavelet transformation for coding noisy
images was decided to belong to CS information systems whereas an
article describing wavelet based corner detection using SVD was
decided to belong to CS artificial intelligence.
For CS artificial intelligence we mostly selected publications 
which mentioned some dimensionality reduction or machine learning 
related term or concept. CS information systems ended up being 
quite like some ``others'' or ``the rest'' dump class.
It would have publications such as
``A reference model for conceptualising the convergence of 
telecommunications and datacommunications service platforms'',
``Developing a distributed meeting service to support mobile 
meeting participants'',
``On voice quality of IP voice over GPRS''.
Decision to assign publications to one discipline only was based 
on using hierarchical and thus hard clustering. Alternatively, 
soft, or probabilistic, clustering would allow a publication to 
be clustered to multiple clusters with varying probabilities. 
% \fixme{Selvennä yläluokka-alaluokka-jako: CS general vs. CS 
% information system tai CS artificial intelligence.}
For clustering the whole data for years 2000-2001 we chose the 
number of clusters based on silhouette values and 
Calinski-Harabasz criterion.



